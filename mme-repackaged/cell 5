# =========================
# Cell 5: SageMaker Pipeline (Notebook-Safe)
# =========================

import boto3
import sagemaker
from sagemaker.workflow.pipeline import Pipeline
from sagemaker.workflow.parameters import ParameterString
from sagemaker.workflow.pipeline_context import PipelineSession
from sagemaker.workflow.functions import Join
from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput
from sagemaker.inputs import TrainingInput
from sagemaker.workflow.steps import ProcessingStep, TrainingStep
from sagemaker.xgboost.estimator import XGBoost
from sagemaker.workflow.step_collections import RegisterModel

role = sagemaker.get_execution_role()
pipeline_session = PipelineSession()

# ---- Pipeline Parameters ----
param_input_csv = ParameterString(
    name="InputCSV",
    default_value=INPUT_S3
)

# ---- Prefixes ----
SPLIT_PREFIX = f"{DATA_PREFIX}/splits"
TRAIN_PREFIX = f"{DATA_PREFIX}/train"
MODEL_PREFIX = f"{DATA_PREFIX}/models"

# ----------------------------------
# STEP 1 — PREPROCESS + FE (per target)
# ----------------------------------

preprocess_steps = {}

for tgt in TARGET_COLS:

    proc = ScriptProcessor(
        image_uri=sagemaker.image_uris.retrieve("sklearn", region, version="1.2-1"),
        command=["python3"],
        instance_type="ml.m5.xlarge",
        instance_count=1,
        role=role,
        base_job_name=f"{PROJECT_NAME}-prep-{tgt}"
    )

    out_prefix = f"{SPLIT_PREFIX}/{tgt}"

    step = ProcessingStep(
        name=f"Preprocess_{tgt}",
        processor=proc,
        code="preprocess_fe.py",
        source_dir=".",
        inputs=[
            ProcessingInput(
                source=param_input_csv,
                destination="/opt/ml/processing/input"
            )
        ],
        outputs=[
            ProcessingOutput(
                output_name="processed",
                source="/opt/ml/processing/output",
                destination=out_prefix
            )
        ],
        job_arguments=[
            "--input_csv", "/opt/ml/processing/input/data.csv",
            "--target", tgt,
            "--output_dir", "/opt/ml/processing/output",
            "--cat_cols", "VendorName",
            "--text_cols", "LineDescription",
            "--num_cols", "ClubNumber",
        ]
    )

    preprocess_steps[tgt] = step


# ----------------------------------
# STEP 2 — TRAIN XGBOOST (per target)
# ----------------------------------

train_steps = {}

for tgt in TARGET_COLS:

    estimator = XGBoost(
        entry_point="train_xgb.py",
        source_dir=".",   # CRITICAL FOR NOTEBOOK
        framework_version="1.3-1",
        py_version="py3",
        role=role,
        instance_type="ml.m5.xlarge",
        instance_count=1,
        output_path=f"{TRAIN_PREFIX}/{tgt}",
        sagemaker_session=pipeline_session
    )

    step = TrainingStep(
        name=f"Train_{tgt}",
        estimator=estimator,
        inputs={
            "train": TrainingInput(
                s3_data=Join(
                    on="/",
                    values=[
                        preprocess_steps[tgt].properties.ProcessingOutputConfig.Outputs["processed"].S3Output.S3Uri,
                        "train.csv",
                    ],
                ),
                content_type="text/csv"
            ),
            "validation": TrainingInput(
                s3_data=Join(
                    on="/",
                    values=[
                        preprocess_steps[tgt].properties.ProcessingOutputConfig.Outputs["processed"].S3Output.S3Uri,
                        "val.csv",
                    ],
                ),
                content_type="text/csv"
            ),
            "fe_model": TrainingInput(
                s3_data=Join(
                    on="/",
                    values=[
                        preprocess_steps[tgt].properties.ProcessingOutputConfig.Outputs["processed"].S3Output.S3Uri,
                        "fe.pkl",
                    ],
                ),
                content_type="application/octet-stream"
            ),
            "label_encoder": TrainingInput(
                s3_data=Join(
                    on="/",
                    values=[
                        preprocess_steps[tgt].properties.ProcessingOutputConfig.Outputs["processed"].S3Output.S3Uri,
                        "label_encoder.pkl",
                    ],
                ),
                content_type="application/octet-stream"
            )
        }
    )

    train_steps[tgt] = step


# ----------------------------------
# STEP 3 — REPACK MODEL (MME-ready)
# ----------------------------------

repack_steps = {}

for tgt in TARGET_COLS:

    repack_proc = ScriptProcessor(
        image_uri=sagemaker.image_uris.retrieve("sklearn", region, version="1.2-1"),
        command=["python3"],
        instance_type="ml.m5.large",
        instance_count=1,
        role=role,
        base_job_name=f"{PROJECT_NAME}-repack-{tgt}",
        source_dir="."
    )

    mme_dest_prefix = f"{MODEL_PREFIX}/{tgt}"

    step = ProcessingStep(
        name=f"Repack_{tgt}",
        processor=repack_proc,
        code="repack_for_mme.py",
        inputs=[
            ProcessingInput(
                source=train_steps[tgt].properties.ModelArtifacts.S3ModelArtifacts,
                destination="/opt/ml/processing/input/model"
            ),
        ],
        outputs=[
            ProcessingOutput(
                output_name="mme_tar",
                source="/opt/ml/processing/output",
                destination=mme_dest_prefix
            )
        ],
        job_arguments=[
            "--model_dir", "/opt/ml/processing/input/model",
            "--output_tar", "/opt/ml/processing/output/model_mme.tar.gz"
        ]
    )

    repack_steps[tgt] = step


# ----------------------------------
# STEP 4 — REGISTER MODEL
# ----------------------------------

register_steps = {}

for tgt in TARGET_COLS:

    register_steps[tgt] = RegisterModel(
        name=f"Register_{tgt}",
        model_data=repack_steps[tgt].properties.ProcessingOutputConfig.Outputs["mme_tar"].S3Output.S3Uri,
        image_uri=sagemaker.image_uris.retrieve("xgboost", region, version="1.3-1"),
        entry_point="inference.py",
        source_dir=".",  # CRITICAL FOR NOTEBOOK
        model_package_group_name=f"{PROJECT_NAME}-{tgt}",
        content_types=["application/json"],
        response_types=["application/json"],
        inference_instances=["ml.m5.large"],
        transform_instances=["ml.m5.large"],
        role=role,
        sagemaker_session=pipeline_session,
    )


# ----------------------------------
# FINAL PIPELINE ASSEMBLY
# ----------------------------------

all_steps = []

for tgt in TARGET_COLS:
    all_steps.append(preprocess_steps[tgt])
    all_steps.append(train_steps[tgt])
    all_steps.append(repack_steps[tgt])
    all_steps.append(register_steps[tgt])

pipeline = Pipeline(
    name=f"{PROJECT_NAME}-pipeline",
    parameters=[param_input_csv],
    steps=all_steps,
    sagemaker_session=pipeline_session
)

print(pipeline.definition())
