import math, time

# Bedrock embedding client
brt = boto3.client("bedrock-runtime", region_name=boto3.Session().region_name)


def embed_batch_cohere(texts):
    # Hard truncate within the batch for safety/throughput
    texts = [t[:MAX_TEXT_CHARS] for t in texts]
    body = {
        "texts": texts,
        "input_type": "search_document",
        "truncate": TRUNCATE_STRATEGY,  # RIGHT truncation on long inputs
        # "output_dimension": 1536,     # uncomment to force vector size (256/512/1024/1536)
        # "embedding_types": ["float"], # default is float; can request int8/uint8/binary/ubinary
    }
    last_err = None
    for attempt in range(1, EMBED_MAX_RETRIES + 1):
        try:
            resp = brt.invoke_model(
                modelId=EMBED_MODEL_ID,
                body=json.dumps(body),
                accept="application/json",
                contentType="application/json",
            )
            payload = json.loads(resp["body"].read())
            emb = payload.get("embeddings")
            # Cohere v4 may return dict keyed by embedding type
            if isinstance(emb, dict):
                for key in ("float", "int8", "uint8", "binary", "ubinary"):
                    if key in emb:
                        emb = emb[key]
                        break
            return emb
        except Exception as e:
            last_err = e
            sleep_s = EMBED_BACKOFF_BASE ** attempt
            time.sleep(sleep_s)
    raise RuntimeError(f"Embedding failed after {EMBED_MAX_RETRIES} retries: {last_err}")


def embed_single(text):
    return embed_batch_cohere([text])[0]


embeddings = []
num_batches = math.ceil(len(texts_ready) / BATCH_SIZE)

for i in tqdm(range(num_batches), desc="Embedding with Bedrock"):
    start = i * BATCH_SIZE
    end = min(start + BATCH_SIZE, len(texts_ready))
    batch = texts_ready[start:end]
    batch_embeddings = embed_batch_cohere(batch)
    embeddings.extend(batch_embeddings)

# Validate/repair embeddings -> float32
cleaned = [None] * len(embeddings)
bad = []
for idx, emb in enumerate(embeddings):
    try:
        cleaned[idx] = np.asarray(emb, dtype=np.float32)
    except Exception as e:
        bad.append((idx, e))

if bad:
    print(f"Retrying {len(bad)} embeddings individually due to parse errors...")
    for idx, err in bad:
        try:
            re_emb = embed_single(texts_ready[idx])
            cleaned[idx] = np.asarray(re_emb, dtype=np.float32)
        except Exception as e2:
            raise RuntimeError(f"Embedding parse failed at global index {idx}: {err}; retry error: {e2}")

embeddings = np.vstack(cleaned)

print(f"Embeddings shape: {embeddings.shape} (records x dimensions)")
assert embeddings.shape[0] == len(df), "Mismatch between embeddings and rows"



# Use existing `embeddings` and `texts_ready` from the long run
cleaned, bad = [None] * len(embeddings), []
for idx, emb in enumerate(embeddings):
    try:
        # unwrap dict responses if present
        if isinstance(emb, dict):
            for key in ("float", "int8", "uint8", "binary", "ubinary"):
                if key in emb:
                    emb = emb[key]
                    break
        cleaned[idx] = np.asarray(emb, dtype=np.float32)
    except Exception as e:
        bad.append((idx, e))

print(f"Needing retry: {len(bad)}")
for idx, err in bad:
    try:
        re_emb = embed_single(texts_ready[idx])  # uses the single-item helper
        cleaned[idx] = np.asarray(re_emb, dtype=np.float32)
    except Exception as e2:
        raise RuntimeError(f"Embedding parse failed at index {idx}: {err}; retry: {e2}")

embeddings = np.vstack(cleaned)
print(embeddings.shape)
