import math

# Bedrock embedding client
brt = boto3.client("bedrock-runtime", region_name=boto3.Session().region_name)


def embed_batch_cohere(texts):
    # Hard truncate within the batch for safety/throughput
    texts = [t[:MAX_TEXT_CHARS] for t in texts]
    body = {
        "texts": texts,
        "input_type": "search_document",
        "truncate": TRUNCATE_STRATEGY,  # RIGHT truncation on long inputs
        # "output_dimension": 1536,     # uncomment to force vector size (256/512/1024/1536)
        # "embedding_types": ["float"], # default is float; can request int8/uint8/binary/ubinary
    }
    resp = brt.invoke_model(
        modelId=EMBED_MODEL_ID,
        body=json.dumps(body),
        accept="application/json",
        contentType="application/json",
    )
    payload = json.loads(resp["body"].read())
    return payload["embeddings"]


embeddings = []
num_batches = math.ceil(len(texts_ready) / BATCH_SIZE)

for i in tqdm(range(num_batches), desc="Embedding with Bedrock"):
    start = i * BATCH_SIZE
    end = min(start + BATCH_SIZE, len(texts_ready))
    batch = texts_ready[start:end]
    batch_embeddings = embed_batch_cohere(batch)
    embeddings.extend(batch_embeddings)

embeddings = np.array(embeddings, dtype=np.float32)

print(f"Embeddings shape: {embeddings.shape} (records x dimensions)")
assert embeddings.shape[0] == len(df), "Mismatch between embeddings and rows"
