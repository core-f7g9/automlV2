{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb91b3b1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Inputs (Studio-friendly) ---\n",
    "import time, io, csv, json, boto3, sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "session = sagemaker.Session()\n",
    "region  = session.boto_region_name\n",
    "bucket  = session.default_bucket()  # Studio-managed default bucket\n",
    "train_key     = \"autopilot-demo/train.csv\"   # CSV with header (includes target)\n",
    "target_col    = \"target\"\n",
    "feature_columns = [\"feature_1\", \"feature_2\", \"feature_3\"]\n",
    "problem_type  = \"MulticlassClassification\"   # or BinaryClassification / Regression\n",
    "objective     = \"Accuracy\"\n",
    "mode          = \"AUTO\"                       # AUTO | ENSEMBLING | HYPERPARAMETER_TUNING\n",
    "instance_type = \"ml.m5.large\"\n",
    "endpoint_name = \"autopilot-poc-endpoint\"\n",
    "\n",
    "s3 = boto3.client(\"s3\", region_name=region)\n",
    "sm = boto3.client(\"sagemaker\", region_name=region)\n",
    "rt = boto3.client(\"sagemaker-runtime\", region_name=region)\n",
    "role = get_execution_role()\n",
    "\n",
    "job_name         = f\"autopilot-{int(time.time())}\"\n",
    "s3_train_path    = f\"s3://{bucket}/{train_key}\"\n",
    "s3_output_path   = f\"s3://{bucket}/autopilot-output/{job_name}\"\n",
    "feature_spec_key = f\"{job_name}/features.json\"\n",
    "\n",
    "# --- Read header & build feature spec (exclude target) ---\n",
    "obj = s3.get_object(Bucket=bucket, Key=train_key)\n",
    "header = next(csv.reader(io.TextIOWrapper(obj[\"Body\"], encoding=\"utf-8\")))\n",
    "assert target_col in header, f\"Target '{target_col}' not found in {header}\"\n",
    "\n",
    "feature_spec = {\"FeatureAttributeNames\": feature_columns}\n",
    "s3.put_object(Bucket=bucket, Key=feature_spec_key, Body=json.dumps(feature_spec).encode(\"utf-8\"))\n",
    "feature_spec_uri = f\"s3://{bucket}/{feature_spec_key}\"\n",
    "\n",
    "# --- Launch AutoML V2 job ---\n",
    "sm.create_auto_ml_job_v2(\n",
    "    AutoMLJobName=job_name,\n",
    "    AutoMLJobInputDataConfig=[{\n",
    "        \"ChannelType\": \"training\",\n",
    "        \"ContentType\": \"text/csv;header=present\",\n",
    "        \"DataSource\": {\"S3DataSource\": {\"S3DataType\": \"S3Prefix\", \"S3Uri\": s3_train_path}}  \n",
    "    }],\n",
    "    OutputDataConfig={\"S3OutputPath\": s3_output_path},\n",
    "    RoleArn=role,\n",
    "    AutoMLJobObjective={\"MetricName\": objective},\n",
    "    AutoMLProblemTypeConfig={\n",
    "        \"TabularJobConfig\": {\n",
    "            \"ProblemType\": problem_type,\n",
    "            \"Mode\": mode,\n",
    "            \"FeatureSpecificationS3Uri\": feature_spec_uri,\n",
    "            \"CompletionCriteria\": {\n",
    "                \"MaxCandidates\": 3,\n",
    "                \"MaxRuntimePerTrainingJobInSeconds\": 1800,\n",
    "                \"MaxAutoMLJobRuntimeInSeconds\": 7200\n",
    "            },\n",
    "            \"TargetAttributeName\": target_col\n",
    "        }\n",
    "    }\n",
    ")\n",
    "print(\"Started:\", job_name)\n",
    "\n",
    "# --- Poll with gentle backoff ---\n",
    "sleep = 30\n",
    "while True:\n",
    "    d = sm.describe_auto_ml_job_v2(AutoMLJobName=job_name)\n",
    "    st = d[\"AutoMLJobStatus\"]; sec = d.get(\"AutoMLJobSecondaryStatus\")\n",
    "    print(\"Status:\", st, \"-\", sec)\n",
    "    if st in (\"Completed\", \"Failed\", \"Stopped\"): break\n",
    "    time.sleep(sleep); sleep = min(sleep + 10, 120)\n",
    "\n",
    "if st != \"Completed\":\n",
    "    raise RuntimeError(f\"AutoML V2 failed: {st} ({sec})\")\n",
    "\n",
    "# --- Deploy best candidate (multi-container aware) ---\n",
    "best = d[\"BestCandidate\"]\n",
    "model_name = f\"{job_name}-model\"\n",
    "cfg_name   = f\"{job_name}-cfg\"\n",
    "\n",
    "sm.create_model(\n",
    "    ModelName=model_name,\n",
    "    Containers=best[\"InferenceContainers\"],  # handles pipelines\n",
    "    ExecutionRoleArn=role\n",
    ")\n",
    "sm.create_endpoint_config(\n",
    "    EndpointConfigName=cfg_name,\n",
    "    ProductionVariants=[{\n",
    "        \"VariantName\": \"AllTraffic\",\n",
    "        \"ModelName\": model_name,\n",
    "        \"InstanceType\": instance_type,\n",
    "        \"InitialInstanceCount\": 1\n",
    "    }]\n",
    ")\n",
    "\n",
    "# Create or update endpoint\n",
    "try:\n",
    "    sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    print(\"Updating endpoint:\", endpoint_name)\n",
    "    sm.update_endpoint(EndpointName=endpoint_name, EndpointConfigName=cfg_name)\n",
    "except sm.exceptions.ResourceNotFound:\n",
    "    print(\"Creating endpoint:\", endpoint_name)\n",
    "    sm.create_endpoint(EndpointName=endpoint_name, EndpointConfigName=cfg_name)\n",
    "\n",
    "sm.get_waiter(\"endpoint_in_service\").wait(EndpointName=endpoint_name)\n",
    "print(\"Endpoint InService:\", endpoint_name)\n",
    "\n",
    "# --- Quick sanity check: build payload in feature_columns order ---\n",
    "obj2 = s3.get_object(Bucket=bucket, Key=train_key)\n",
    "lines = obj2[\"Body\"].read().decode(\"utf-8\").splitlines()\n",
    "if len(lines) > 1:\n",
    "    row = dict(zip(header, next(csv.reader([lines[1]]))))\n",
    "    payload = \",\".join([row.get(col, \"\") for col in feature_columns]) + \"\\n\"\n",
    "    resp = rt.invoke_endpoint(EndpointName=endpoint_name, ContentType=\"text/csv\", Body=payload)\n",
    "    print(\"Sample prediction:\", resp[\"Body\"].read().decode(\"utf-8\"))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
