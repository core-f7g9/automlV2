{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb91b3b1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Inputs ---\n",
    "import time, io, csv, json, boto3, botocore, sagemaker, sys\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "region        = boto3.Session().region_name\n",
    "bucket        = \"your-s3-bucket\"\n",
    "train_key     = \"data/your-training.csv\"     # CSV with header (includes target)\n",
    "target_col    = \"target\"\n",
    "problem_type  = \"MulticlassClassification\"   # or BinaryClassification / Regression\n",
    "objective     = \"F1Macro\"                    # pick an appropriate metric\n",
    "mode          = \"AUTO\"                       # AUTO | ENSEMBLING | HYPERPARAMETER_TUNING\n",
    "instance_type = \"ml.m5.large\"\n",
    "endpoint_name = \"autopilot-poc-endpoint\"\n",
    "\n",
    "s3 = boto3.client(\"s3\", region_name=region)\n",
    "sm = boto3.client(\"sagemaker\", region_name=region)\n",
    "rt = boto3.client(\"sagemaker-runtime\", region_name=region)\n",
    "\n",
    "# get_execution_role() works in SageMaker environments; fall back otherwise\n",
    "try:\n",
    "    role = get_execution_role()\n",
    "except Exception:\n",
    "    role = \"arn:aws:iam::<ACCOUNT_ID>:role/<SageMakerExecutionRole>\"\n",
    "\n",
    "job_name        = f\"autopilot-{int(time.time())}\"\n",
    "s3_train_path   = f\"s3://{bucket}/{train_key}\"\n",
    "s3_output_path  = f\"s3://{bucket}/autopilot-output/{job_name}\"\n",
    "feature_spec_key = f\"{job_name}/features.json\"\n",
    "\n",
    "# --- Read CSV header safely ---\n",
    "obj = s3.get_object(Bucket=bucket, Key=train_key)\n",
    "header = next(csv.reader(io.TextIOWrapper(obj[\"Body\"], encoding=\"utf-8\")))\n",
    "if target_col not in header:\n",
    "    raise ValueError(f\"Target column '{target_col}' not found in header: {header}\")\n",
    "\n",
    "# Build feature list in deterministic order (header minus target)\n",
    "feature_columns = [c for c in header if c != target_col]\n",
    "\n",
    "# --- Feature spec (no TargetAttributeName here) ---\n",
    "feature_spec = {\n",
    "    \"FeatureAttributeNames\": feature_columns,\n",
    "    # Optional: \"FeatureDataTypes\": [{\"FeatureName\":\"colA\",\"FeatureDataType\":\"String\"}, ...]\n",
    "}\n",
    "s3.put_object(Bucket=bucket, Key=feature_spec_key, Body=json.dumps(feature_spec).encode(\"utf-8\"))\n",
    "feature_spec_uri = f\"s3://{bucket}/{feature_spec_key}\"\n",
    "\n",
    "# --- Launch AutoML V2 job (explicit objective + mode + completion caps) ---\n",
    "sm.create_auto_ml_job_v2(\n",
    "    AutoMLJobName=job_name,\n",
    "    AutoMLJobInputDataConfig=[{\n",
    "        \"ChannelType\": \"training\",\n",
    "        \"ContentType\": \"text/csv;header=present\",\n",
    "        \"DataSource\": {\"S3DataSource\": {\"S3DataType\": \"S3Prefix\", \"S3Uri\": s3_train_path}},\n",
    "        \"TargetAttributeName\": target_col\n",
    "    }],\n",
    "    AutoMLJobOutputDataConfig={\"S3OutputPath\": s3_output_path},\n",
    "    RoleArn=role,\n",
    "    AutoMLJobObjective={\"MetricName\": objective},\n",
    "    AutoMLProblemTypeConfig={\n",
    "        \"TabularJobConfig\": {\n",
    "            \"ProblemType\": problem_type,\n",
    "            \"Mode\": mode,\n",
    "            \"FeatureSpecificationS3Uri\": feature_spec_uri,\n",
    "            \"CompletionCriteria\": {\n",
    "                \"MaxCandidates\": 3,                          # keep tiny for PoC\n",
    "                \"MaxRuntimePerTrainingJobInSeconds\": 1800,   # 30 min/job\n",
    "                \"MaxAutoMLJobRuntimeInSeconds\": 7200         # 2 hours total\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "print(\"Started:\", job_name)\n",
    "\n",
    "# --- Poll with simple backoff ---\n",
    "sleep = 30\n",
    "deadline = time.time() + 3*3600\n",
    "while True:\n",
    "    d = sm.describe_auto_ml_job_v2(AutoMLJobName=job_name)\n",
    "    st = d[\"AutoMLJobStatus\"]; sec = d.get(\"AutoMLJobSecondaryStatus\")\n",
    "    print(\"Status:\", st, \"-\", sec)\n",
    "    if st in (\"Completed\", \"Failed\", \"Stopped\"):\n",
    "        break\n",
    "    if time.time() > deadline:\n",
    "        raise TimeoutError(\"AutoML job exceeded local wait deadline.\")\n",
    "    time.sleep(sleep)\n",
    "    if sleep < 120:\n",
    "        sleep += 10  # gentle backoff\n",
    "\n",
    "if st != \"Completed\":\n",
    "    raise RuntimeError(f\"AutoML V2 failed: {st} ({sec})\")\n",
    "\n",
    "# --- Best candidate & deploy (multi-container aware) ---\n",
    "best = d[\"BestCandidate\"]\n",
    "model_name = f\"{job_name}-model\"\n",
    "cfg_name   = f\"{job_name}-cfg\"\n",
    "\n",
    "sm.create_model(\n",
    "    ModelName=model_name,\n",
    "    Containers=best[\"InferenceContainers\"],  # Handles pipelines automatically\n",
    "    ExecutionRoleArn=role\n",
    "    # Optionally: EnableNetworkIsolation=True, VpcConfig={...}\n",
    ")\n",
    "\n",
    "sm.create_endpoint_config(\n",
    "    EndpointConfigName=cfg_name,\n",
    "    ProductionVariants=[{\n",
    "        \"VariantName\": \"AllTraffic\",\n",
    "        \"ModelName\": model_name,\n",
    "        \"InstanceType\": instance_type,\n",
    "        \"InitialInstanceCount\": 1\n",
    "    }]\n",
    ")\n",
    "\n",
    "# Create-or-update endpoint (narrow exception to ResourceNotFound)\n",
    "try:\n",
    "    sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    print(\"Updating endpoint:\", endpoint_name)\n",
    "    sm.update_endpoint(EndpointName=endpoint_name, EndpointConfigName=cfg_name)\n",
    "except sm.exceptions.ResourceNotFound:\n",
    "    print(\"Creating endpoint:\", endpoint_name)\n",
    "    sm.create_endpoint(EndpointName=endpoint_name, EndpointConfigName=cfg_name)\n",
    "\n",
    "waiter = sm.get_waiter(\"endpoint_in_service\")\n",
    "waiter.wait(EndpointName=endpoint_name)\n",
    "print(\"Endpoint InService:\", endpoint_name)\n",
    "\n",
    "# --- Quick test: build payload in *feature_columns* order ---\n",
    "obj2 = s3.get_object(Bucket=bucket, Key=train_key)\n",
    "lines = obj2[\"Body\"].read().decode(\"utf-8\").splitlines()\n",
    "if len(lines) > 1:\n",
    "    sample = dict(zip(header, next(csv.reader([lines[1]]))))\n",
    "    payload_row = [sample.get(col, \"\") for col in feature_columns]  # exact order\n",
    "    payload = \",\".join(payload_row) + \"\\n\"\n",
    "    resp = rt.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"text/csv\",\n",
    "        Body=payload.encode(\"utf-8\")\n",
    "    )\n",
    "    print(\"Sample prediction:\", resp[\"Body\"].read().decode(\"utf-8\"))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
