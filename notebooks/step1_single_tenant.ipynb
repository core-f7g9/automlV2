{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb91b3b1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Inputs (Studio-friendly) ---\n",
    "import time, io, csv, json, boto3, sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "session = sagemaker.Session()\n",
    "region  = session.boto_region_name\n",
    "bucket  = session.default_bucket()  # Studio-managed default bucket\n",
    "train_key     = \"autopilot-demo/train.csv\"   # CSV with header (includes target)\n",
    "target_col    = \"target\"\n",
    "feature_columns = [\"feature_1\", \"feature_2\", \"feature_3\"]\n",
    "problem_type  = \"MulticlassClassification\"   # or BinaryClassification / Regression\n",
    "objective     = \"Accuracy\"\n",
    "mode          = \"AUTO\"                       # AUTO | ENSEMBLING | HYPERPARAMETER_TUNING\n",
    "instance_type = \"ml.m5.large\"\n",
    "endpoint_name = \"autopilot-poc-endpoint\"\n",
    "\n",
    "s3 = boto3.client(\"s3\", region_name=region)\n",
    "sm = boto3.client(\"sagemaker\", region_name=region)\n",
    "rt = boto3.client(\"sagemaker-runtime\", region_name=region)\n",
    "role = get_execution_role()\n",
    "\n",
    "job_name         = f\"autopilot-{int(time.time())}\"\n",
    "s3_train_path    = f\"s3://{bucket}/{train_key}\"\n",
    "s3_output_path   = f\"s3://{bucket}/autopilot-output/{job_name}\"\n",
    "feature_spec_key = f\"{job_name}/features.json\"\n",
    "\n",
    "# --- Read header & build feature spec (exclude target) ---\n",
    "obj = s3.get_object(Bucket=bucket, Key=train_key)\n",
    "header = next(csv.reader(io.TextIOWrapper(obj[\"Body\"], encoding=\"utf-8\")))\n",
    "assert target_col in header, f\"Target '{target_col}' not found in {header}\"\n",
    "\n",
    "feature_spec = {\"FeatureAttributeNames\": feature_columns}\n",
    "s3.put_object(Bucket=bucket, Key=feature_spec_key, Body=json.dumps(feature_spec).encode(\"utf-8\"))\n",
    "feature_spec_uri = f\"s3://{bucket}/{feature_spec_key}\"\n",
    "\n",
    "# --- Launch AutoML V2 job ---\n",
    "sm.create_auto_ml_job_v2(\n",
    "    AutoMLJobName=job_name,\n",
    "    AutoMLJobInputDataConfig=[{\n",
    "        \"ChannelType\": \"training\",\n",
    "        \"ContentType\": \"text/csv;header=present\",\n",
    "        \"DataSource\": {\"S3DataSource\": {\"S3DataType\": \"S3Prefix\", \"S3Uri\": s3_train_path}}  \n",
    "    }],\n",
    "    OutputDataConfig={\"S3OutputPath\": s3_output_path},\n",
    "    RoleArn=role,\n",
    "    AutoMLJobObjective={\"MetricName\": objective},\n",
    "    AutoMLProblemTypeConfig={\n",
    "        \"TabularJobConfig\": {\n",
    "            \"ProblemType\": problem_type,\n",
    "            \"Mode\": mode,\n",
    "            \"FeatureSpecificationS3Uri\": feature_spec_uri,\n",
    "            \"CompletionCriteria\": {\n",
    "                \"MaxCandidates\": 3,\n",
    "                \"MaxRuntimePerTrainingJobInSeconds\": 1800,\n",
    "                \"MaxAutoMLJobRuntimeInSeconds\": 7200\n",
    "            },\n",
    "            \"TargetAttributeName\": target_col\n",
    "        }\n",
    "    }\n",
    ")\n",
    "print(\"Started:\", job_name)\n",
    "\n",
    "# --- Poll with gentle backoff ---\n",
    "sleep = 30\n",
    "while True:\n",
    "    d = sm.describe_auto_ml_job_v2(AutoMLJobName=job_name)\n",
    "    st = d[\"AutoMLJobStatus\"]; sec = d.get(\"AutoMLJobSecondaryStatus\")\n",
    "    print(\"Status:\", st, \"-\", sec)\n",
    "    if st in (\"Completed\", \"Failed\", \"Stopped\"): break\n",
    "    time.sleep(sleep); sleep = min(sleep + 10, 120)\n",
    "\n",
    "if st != \"Completed\":\n",
    "    raise RuntimeError(f\"AutoML V2 failed: {st} ({sec})\")\n",
    "\n",
    "# --- Deploy best candidate (multi-container aware) ---\n",
    "best = d[\"BestCandidate\"]\n",
    "model_name = f\"{job_name}-model\"\n",
    "cfg_name   = f\"{job_name}-cfg\"\n",
    "\n",
    "sm.create_model(\n",
    "    ModelName=model_name,\n",
    "    Containers=best[\"InferenceContainers\"],  # handles pipelines\n",
    "    ExecutionRoleArn=role\n",
    ")\n",
    "sm.create_endpoint_config(\n",
    "    EndpointConfigName=cfg_name,\n",
    "    ProductionVariants=[{\n",
    "        \"VariantName\": \"AllTraffic\",\n",
    "        \"ModelName\": model_name,\n",
    "        \"InstanceType\": instance_type,\n",
    "        \"InitialInstanceCount\": 1\n",
    "    }]\n",
    ")\n",
    "\n",
    "# Create or update endpoint\n",
    "try:\n",
    "    sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    print(\"Updating endpoint:\", endpoint_name)\n",
    "    sm.update_endpoint(EndpointName=endpoint_name, EndpointConfigName=cfg_name)\n",
    "except sm.exceptions.ResourceNotFound:\n",
    "    print(\"Creating endpoint:\", endpoint_name)\n",
    "    sm.create_endpoint(EndpointName=endpoint_name, EndpointConfigName=cfg_name)\n",
    "\n",
    "sm.get_waiter(\"endpoint_in_service\").wait(EndpointName=endpoint_name)\n",
    "print(\"Endpoint InService:\", endpoint_name)\n",
    "\n",
    "# --- Quick sanity check: build payload in feature_columns order ---\n",
    "resp = s3.get_object(Bucket=bucket, Key=train_key)\n",
    "csv_bytes = resp[\"Body\"].read()          # fully load\n",
    "text_stream = io.StringIO(csv_bytes.decode(\"utf-8\"))\n",
    "\n",
    "reader = csv.reader(text_stream)\n",
    "header = next(reader)\n",
    "\n",
    "# take first 1000 records (or fewer if file smaller)\n",
    "rows = [row for _, row in zip(range(1000), reader)]\n",
    "\n",
    "# remove target column (keep others)\n",
    "target_index = header.index(target_col)\n",
    "clean_rows = [[v for i, v in enumerate(r) if i != target_index] for r in rows]\n",
    "\n",
    "# build payload: join the first record for single prediction (or join all if needed)\n",
    "payload = \",\".join(clean_rows[0])\n",
    "\n",
    "print(f\"Read {len(rows)} rows, sending {len(clean_rows[0])} columns (excluding target).\")\n",
    "\n",
    "runtime = boto3.client(\"sagemaker-runtime\", region_name=region)\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"text/csv\",\n",
    "    Body=payload.encode(\"utf-8\"),\n",
    ")\n",
    "print(\"Sample prediction:\", response[\"Body\"].read().decode())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
