import sagemaker
import boto3
from sagemaker.workflow.pipeline import Pipeline
from sagemaker.workflow.steps import ProcessingStep
from sagemaker.workflow.automl_step import AutoMLStep
from sagemaker.sklearn.processing import SKLearnProcessor
from sagemaker.processing import ProcessingInput, ProcessingOutput
from sagemaker.automl.automl import AutoML
from sagemaker.workflow.pipeline_context import PipelineSession

# --- CONFIGURATION ---
TARGET_COLS = ["DepartmentCode", "AccountCode", "SubAccountCode", "LocationCode"]
INPUT_FEATURES = ["VendorName", "LineDescription", "ClubNumber"]

# S3 Location of your existing master 2M record CSV
INPUT_DATA_URI = "s3://your-bucket-name/path/to/master_dataset.csv" 
BUCKET = sagemaker.Session().default_bucket()
ROLE = sagemaker.get_execution_role()
PREFIX = "sagemaker/automl-dynamic-pipeline"

# Session setup
pipeline_session = PipelineSession()

# --- STEP 1: PROCESSING (Splits data to prevent leakage) ---
processor = SKLearnProcessor(
    framework_version="1.0-1",
    instance_type="ml.m5.2xlarge", # 2xlarge to handle 2M rows comfortably
    instance_count=1,
    base_job_name="automl-splitter",
    role=ROLE,
    sagemaker_session=pipeline_session
)

# Dynamically generate outputs: One S3 folder per target
processing_outputs = []
for target in TARGET_COLS:
    processing_outputs.append(
        ProcessingOutput(
            output_name=target, 
            source=f"/opt/ml/processing/output/{target}",
            destination=f"s3://{BUCKET}/{PREFIX}/data/{target}"
        )
    )

step_process = ProcessingStep(
    name="SplitDataForTargets",
    processor=processor,
    inputs=[
        ProcessingInput(source=INPUT_DATA_URI, destination="/opt/ml/processing/input", input_name="input_data")
    ],
    outputs=processing_outputs,
    code="preprocess.py", # The script we wrote above
    job_arguments=[
        "--targets", ",".join(TARGET_COLS),
        "--features", ",".join(INPUT_FEATURES)
    ]
)

# --- STEP 2: AUTOML STEPS (Corrected for Modern SDK) ---
automl_steps = []

for target in TARGET_COLS:
    # 1. Define the AutoML configuration
    # Note: We must explicitly set auto_generate_endpoint_name=False or handle naming to avoid conflicts in loops
    automl = AutoML(
        role=ROLE,
        target_attribute_name=target,
        output_path=f"s3://{BUCKET}/{PREFIX}/output/{target}",
        max_candidates=10,
        total_job_runtime_in_seconds=3600,
        mode="ENSEMBLING",
        sagemaker_session=pipeline_session
    )
    
    # 2. Define the input source from the previous Processing Step
    input_data_uri = step_process.properties.ProcessingOutputConfig.Outputs[target].S3Output.S3Uri
    
    # 3. Generate the step arguments by calling .fit()
    # IMPORTANT: We do not actually run training here. Because we use 'pipeline_session',
    # this call merely records the arguments needed for the step.
    automl_args = automl.fit(
        inputs=input_data_uri,
        wait=False, 
        logs=False
    )
    
    # 4. Create the AutoMLStep using 'step_args'
    step_automl = AutoMLStep(
        name=f"Train-{target}",
        step_args=automl_args
    )
    
    automl_steps.append(step_automl)

# --- STEP 3: BUILD AND RUN PIPELINE ---
pipeline = Pipeline(
    name="Dynamic-MultiTarget-AutoML",
    # We flatten the list so it is [ProcessingStep, AutoMLStep1, AutoMLStep2, ...]
    steps=[step_process] + automl_steps, 
    sagemaker_session=pipeline_session
)

# Create/Update the pipeline
pipeline.upsert(role_arn=ROLE)

# Execute
execution = pipeline.start()
print(f"Pipeline started! Execution ARN: {execution.arn}")