import sagemaker
import boto3
from sagemaker.workflow.pipeline import Pipeline
from sagemaker.workflow.steps import ProcessingStep
from sagemaker.workflow.automl_step import AutoMLStep
from sagemaker.sklearn.processing import SKLearnProcessor
from sagemaker.processing import ProcessingInput, ProcessingOutput
from sagemaker.automl.automl import AutoML
from sagemaker.workflow.pipeline_context import PipelineSession

# --- CONFIGURATION ---
TARGET_COLS = ["DepartmentCode", "AccountCode", "SubAccountCode", "LocationCode"]
INPUT_FEATURES = ["VendorName", "LineDescription", "ClubNumber"]

# S3 Location of your existing master 2M record CSV
INPUT_DATA_URI = "s3://your-bucket-name/path/to/master_dataset.csv" 
BUCKET = sagemaker.Session().default_bucket()
ROLE = sagemaker.get_execution_role()
PREFIX = "sagemaker/automl-dynamic-pipeline"

# Session setup
pipeline_session = PipelineSession()

# --- STEP 1: PROCESSING (Splits data to prevent leakage) ---
processor = SKLearnProcessor(
    framework_version="1.0-1",
    instance_type="ml.m5.2xlarge", # 2xlarge to handle 2M rows comfortably
    instance_count=1,
    base_job_name="automl-splitter",
    role=ROLE,
    sagemaker_session=pipeline_session
)

# Dynamically generate outputs: One S3 folder per target
processing_outputs = []
for target in TARGET_COLS:
    processing_outputs.append(
        ProcessingOutput(
            output_name=target, 
            source=f"/opt/ml/processing/output/{target}",
            destination=f"s3://{BUCKET}/{PREFIX}/data/{target}"
        )
    )

step_process = ProcessingStep(
    name="SplitDataForTargets",
    processor=processor,
    inputs=[
        ProcessingInput(source=INPUT_DATA_URI, destination="/opt/ml/processing/input", input_name="input_data")
    ],
    outputs=processing_outputs,
    code="preprocess.py", # The script we wrote above
    job_arguments=[
        "--targets", ",".join(TARGET_COLS),
        "--features", ",".join(INPUT_FEATURES)
    ]
)

# --- STEP 2: AUTOML STEPS (Fixed for Pipeline Variables) ---
automl_steps = []

for target in TARGET_COLS:
    # 1. Define AutoML Config
    automl = AutoML(
        role=ROLE,
        target_attribute_name=target,
        output_path=f"s3://{BUCKET}/{PREFIX}/output/{target}",
        max_candidates=10,
        total_job_runtime_in_seconds=3600,
        mode="ENSEMBLING",
        sagemaker_session=pipeline_session
    )
    
    # 2. Capture the Pipeline Variable (The connection to the previous step)
    input_data_node = step_process.properties.ProcessingOutputConfig.Outputs[target].S3Output.S3Uri
    
    # 3. Generate args with a DUMMY input
    # We pass None or a dummy list here to bypass the Python-side validation in .fit()
    # The real input will be injected in the next step.
    automl_args = automl.fit(
        inputs=None, 
        wait=False, 
        logs=False
    )
    
    # 4. Create the Step with the REAL input
    # 'inputs' here overrides whatever was in .fit() and handles the Pipeline Variable correctly
    step_automl = AutoMLStep(
        name=f"Train-{target}",
        step_args=automl_args,
        inputs=[input_data_node] 
    )
    
    automl_steps.append(step_automl)

# --- STEP 3: BUILD AND RUN ---
pipeline = Pipeline(
    name="Dynamic-MultiTarget-AutoML",
    steps=[step_process] + automl_steps, 
    sagemaker_session=pipeline_session
)

pipeline.upsert(role_arn=ROLE)
execution = pipeline.start()
print(f"Pipeline started! Execution ARN: {execution.arn}")