import sagemaker
import boto3
from sagemaker.workflow.pipeline import Pipeline
from sagemaker.workflow.steps import ProcessingStep
from sagemaker.workflow.automl_step import AutoMLStep
from sagemaker.sklearn.processing import SKLearnProcessor
from sagemaker.processing import ProcessingInput, ProcessingOutput
from sagemaker.automl.automl import AutoML
from sagemaker.workflow.pipeline_context import PipelineSession

# --- CONFIGURATION ---
TARGET_COLS = ["DepartmentCode", "AccountCode", "SubAccountCode", "LocationCode"]
INPUT_FEATURES = ["VendorName", "LineDescription", "ClubNumber"]

# S3 Location of your existing master 2M record CSV
INPUT_DATA_URI = "s3://your-bucket-name/path/to/master_dataset.csv" 
BUCKET = sagemaker.Session().default_bucket()
ROLE = sagemaker.get_execution_role()
PREFIX = "sagemaker/automl-dynamic-pipeline"

# Session setup
pipeline_session = PipelineSession()

# --- STEP 1: PROCESSING (Splits data to prevent leakage) ---
processor = SKLearnProcessor(
    framework_version="1.0-1",
    instance_type="ml.m5.2xlarge", # 2xlarge to handle 2M rows comfortably
    instance_count=1,
    base_job_name="automl-splitter",
    role=ROLE,
    sagemaker_session=pipeline_session
)

# Dynamically generate outputs: One S3 folder per target
processing_outputs = []
for target in TARGET_COLS:
    processing_outputs.append(
        ProcessingOutput(
            output_name=target, 
            source=f"/opt/ml/processing/output/{target}",
            destination=f"s3://{BUCKET}/{PREFIX}/data/{target}"
        )
    )

step_process = ProcessingStep(
    name="SplitDataForTargets",
    processor=processor,
    inputs=[
        ProcessingInput(source=INPUT_DATA_URI, destination="/opt/ml/processing/input", input_name="input_data")
    ],
    outputs=processing_outputs,
    code="preprocess.py", # The script we wrote above
    job_arguments=[
        "--targets", ",".join(TARGET_COLS),
        "--features", ",".join(INPUT_FEATURES)
    ]
)

# --- STEP 2: AUTOML STEPS (Dynamic Loop) ---
automl_steps = []

for target in TARGET_COLS:
    # 1. Define the AutoML configuration for this specific target
    automl = AutoML(
        role=ROLE,
        target_attribute_name=target,
        output_path=f"s3://{BUCKET}/{PREFIX}/output/{target}",
        max_candidates=10,             # Keep it low for testing, increase for production
        total_job_runtime_in_seconds=3600, # 1 hour max per job
        mode="ENSEMBLING",             # Ensembling is usually faster/cheaper for large tabular data
        sagemaker_session=pipeline_session
    )
    
    # 2. Define the inputs (The specific output from the processing step)
    # We map the Processing Step's output directly to this AutoML step's input
    input_data = step_process.properties.ProcessingOutputConfig.Outputs[target].S3Output.S3Uri
    
    # 3. Create the Pipeline Step
    step_automl = AutoMLStep(
        name=f"Train-{target}",
        automl=automl,
        inputs=input_data
    )
    
    automl_steps.append(step_automl)

# --- STEP 3: BUILD AND RUN PIPELINE ---
pipeline = Pipeline(
    name="Dynamic-MultiTarget-AutoML",
    steps=[step_process] + automl_steps, # Run Process first, then all AutoMLs run in parallel
    sagemaker_session=pipeline_session
)

# Create/Update the pipeline
pipeline.upsert(role_arn=ROLE)

# Execute
execution = pipeline.start()
print(f"Pipeline started! Execution ARN: {execution.arn}")