"""
SageMaker Autopilot Pipeline for Multi-Class Classification with Multiple Targets
This pipeline trains separate models for each target column dynamically
"""

import boto3
import sagemaker
from sagemaker import AutoML
from sagemaker.session import Session
import pandas as pd
import time
from datetime import datetime
import json

# ============================================================================
# CONFIGURATION - Modify these parameters
# ============================================================================

# Define your target columns and input features
TARGET_COLS = ["DepartmentCode", "AccountCode", "SubAccountCode", "LocationCode"]
INPUT_FEATURES = ["VendorName", "LineDescription", "ClubNumber"]

# S3 paths
BUCKET_NAME = "your-bucket-name"  # Replace with your S3 bucket
INPUT_DATA_PREFIX = "input-data"  # S3 prefix where your CSV is stored
OUTPUT_PREFIX = "autopilot-output"  # S3 prefix for outputs

# Dataset file name
DATASET_FILE = "your_dataset.csv"  # Replace with your CSV file name

# AutoML Job Configuration
MAX_CANDIDATES = 50  # Maximum number of model candidates per target
MAX_RUNTIME_PER_JOB = 3600  # Max runtime per job in seconds (1 hour)
PROBLEM_TYPE = "MulticlassClassification"  # Can be auto-detected or specified
OBJECTIVE_METRIC = "Accuracy"  # Metric for multi-class classification

# Training mode: 'ENSEMBLING' (fast, <100MB), 'HPO' (larger datasets), or 'AUTO'
TRAINING_MODE = "AUTO"

# IAM Role - SageMaker execution role
ROLE = sagemaker.get_execution_role()

# ============================================================================
# INITIALIZE AWS CLIENTS
# ============================================================================

sm_client = boto3.client('sagemaker')
s3_client = boto3.client('s3')
sagemaker_session = Session()

print(f"Using SageMaker role: {ROLE}")
print(f"Default S3 bucket: {BUCKET_NAME}")
print(f"Region: {sagemaker_session.boto_region_name}")

# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

def wait_for_autopilot_job(job_name, check_interval=60):
    """
    Monitor AutoML job status until completion
    """
    print(f"\nMonitoring job: {job_name}")
    
    while True:
        response = sm_client.describe_auto_ml_job(AutoMLJobName=job_name)
        status = response['AutoMLJobStatus']
        secondary_status = response.get('AutoMLJobSecondaryStatus', 'N/A')
        
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        print(f"[{timestamp}] Status: {status} | Secondary: {secondary_status}")
        
        if status in ['Completed', 'Failed', 'Stopped']:
            if status == 'Completed':
                print(f"\n✓ Job {job_name} completed successfully!")
                best_candidate = response.get('BestCandidate')
                if best_candidate:
                    metric_name = best_candidate['FinalAutoMLJobObjectiveMetric']['MetricName']
                    metric_value = best_candidate['FinalAutoMLJobObjectiveMetric']['Value']
                    print(f"  Best {metric_name}: {metric_value:.4f}")
                    print(f"  Best Candidate: {best_candidate['CandidateName']}")
                return response
            else:
                failure_reason = response.get('FailureReason', 'Unknown')
                print(f"\n✗ Job {job_name} ended with status: {status}")
                print(f"  Reason: {failure_reason}")
                return response
        
        time.sleep(check_interval)


def get_column_info(df, target_col):
    """
    Get information about target column for problem type inference
    """
    unique_values = df[target_col].nunique()
    print(f"\nTarget column '{target_col}' statistics:")
    print(f"  Unique values: {unique_values}")
    print(f"  Missing values: {df[target_col].isnull().sum()}")
    print(f"  Sample values: {df[target_col].unique()[:10].tolist()}")
    return unique_values


def prepare_dataset_for_target(df, target_col, input_features, output_path):
    """
    Prepare dataset with specific target and input features
    Returns S3 path of prepared dataset
    """
    # Select input features and target column
    columns_to_keep = input_features + [target_col]
    df_target = df[columns_to_keep].copy()
    
    # Remove rows with missing target values
    df_target = df_target.dropna(subset=[target_col])
    
    print(f"\nPreparing dataset for target: {target_col}")
    print(f"  Shape: {df_target.shape}")
    print(f"  Features: {input_features}")
    
    # Save to local CSV
    local_file = f"train_{target_col}.csv"
    df_target.to_csv(local_file, index=False, header=True)
    
    # Upload to S3
    s3_path = f"s3://{BUCKET_NAME}/{output_path}/{local_file}"
    sagemaker_session.upload_data(
        path=local_file,
        bucket=BUCKET_NAME,
        key_prefix=output_path
    )
    
    print(f"  Uploaded to: {s3_path}")
    return s3_path, df_target.shape[0]


def create_autopilot_job(target_col, input_s3_path, output_s3_path, job_name):
    """
    Create and launch an Autopilot job for a specific target
    """
    print(f"\n{'='*80}")
    print(f"Creating Autopilot job: {job_name}")
    print(f"{'='*80}")
    
    # Input data configuration
    input_data_config = [{
        'DataSource': {
            'S3DataSource': {
                'S3DataType': 'S3Prefix',
                'S3Uri': input_s3_path
            }
        },
        'TargetAttributeName': target_col
    }]
    
    # Output data configuration
    output_data_config = {
        'S3OutputPath': output_s3_path
    }
    
    # Job configuration
    auto_ml_job_config = {
        'CompletionCriteria': {
            'MaxCandidates': MAX_CANDIDATES,
            'MaxRuntimePerTrainingJobInSeconds': MAX_RUNTIME_PER_JOB,
            'MaxAutoMLJobRuntimeInSeconds': MAX_RUNTIME_PER_JOB * 3
        }
    }
    
    # AutoML job objective
    auto_ml_job_objective = {
        'MetricName': OBJECTIVE_METRIC
    }
    
    # Create job parameters
    job_params = {
        'AutoMLJobName': job_name,
        'InputDataConfig': input_data_config,
        'OutputDataConfig': output_data_config,
        'AutoMLJobConfig': auto_ml_job_config,
        'AutoMLJobObjective': auto_ml_job_objective,
        'RoleArn': ROLE
    }
    
    # Add problem type if specified (not 'AUTO')
    if PROBLEM_TYPE != 'AUTO':
        job_params['ProblemType'] = PROBLEM_TYPE
    
    # Add training mode
    if TRAINING_MODE in ['ENSEMBLING', 'HPO']:
        job_params['AutoMLJobConfig']['Mode'] = TRAINING_MODE
    
    # Create the job
    try:
        response = sm_client.create_auto_ml_job(**job_params)
        print(f"✓ Job created successfully!")
        print(f"  Job ARN: {response['AutoMLJobArn']}")
        return job_name
    except Exception as e:
        print(f"✗ Error creating job: {str(e)}")
        raise


def get_best_model_info(job_name):
    """
    Retrieve information about the best model from completed job
    """
    response = sm_client.describe_auto_ml_job(AutoMLJobName=job_name)
    
    if response['AutoMLJobStatus'] != 'Completed':
        print(f"Job {job_name} is not completed yet")
        return None
    
    best_candidate = response.get('BestCandidate')
    if not best_candidate:
        print(f"No best candidate found for job {job_name}")
        return None
    
    model_info = {
        'job_name': job_name,
        'candidate_name': best_candidate['CandidateName'],
        'objective_metric_name': best_candidate['FinalAutoMLJobObjectiveMetric']['MetricName'],
        'objective_metric_value': best_candidate['FinalAutoMLJobObjectiveMetric']['Value'],
        'inference_containers': best_candidate['InferenceContainers'],
        'creation_time': response['CreationTime'],
        'end_time': response.get('EndTime')
    }
    
    return model_info

# ============================================================================
# MAIN PIPELINE EXECUTION
# ============================================================================

def run_autopilot_pipeline():
    """
    Main function to run the complete pipeline for all target columns
    """
    print("\n" + "="*80)
    print("SAGEMAKER AUTOPILOT MULTI-TARGET CLASSIFICATION PIPELINE")
    print("="*80)
    
    timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')
    
    # Step 1: Load data from S3
    print(f"\n[Step 1] Loading data from S3...")
    input_s3_path = f"s3://{BUCKET_NAME}/{INPUT_DATA_PREFIX}/{DATASET_FILE}"
    print(f"Reading from: {input_s3_path}")
    
    try:
        df = pd.read_csv(input_s3_path)
        print(f"✓ Data loaded successfully!")
        print(f"  Shape: {df.shape}")
        print(f"  Columns: {df.columns.tolist()}")
    except Exception as e:
        print(f"✗ Error loading data: {str(e)}")
        print(f"\nMake sure your CSV file is uploaded to: {input_s3_path}")
        return
    
    # Validate columns
    missing_features = set(INPUT_FEATURES) - set(df.columns)
    missing_targets = set(TARGET_COLS) - set(df.columns)
    
    if missing_features:
        print(f"\n✗ Missing input features: {missing_features}")
        return
    if missing_targets:
        print(f"\n✗ Missing target columns: {missing_targets}")
        return
    
    print(f"✓ All required columns found!")
    
    # Step 2: Analyze and prepare datasets for each target
    print(f"\n[Step 2] Preparing datasets for each target...")
    
    prepared_datasets = {}
    for target_col in TARGET_COLS:
        # Get column information
        unique_values = get_column_info(df, target_col)
        
        # Prepare dataset
        prep_prefix = f"{OUTPUT_PREFIX}/prepared-data/{timestamp}"
        s3_path, row_count = prepare_dataset_for_target(
            df, target_col, INPUT_FEATURES, prep_prefix
        )
        
        prepared_datasets[target_col] = {
            's3_path': s3_path,
            'row_count': row_count,
            'unique_values': unique_values
        }
    
    # Step 3: Launch Autopilot jobs for each target
    print(f"\n[Step 3] Launching Autopilot jobs...")
    
    job_info = {}
    for target_col in TARGET_COLS:
        job_name = f"autopilot-{target_col.lower()}-{timestamp}"
        output_s3_path = f"s3://{BUCKET_NAME}/{OUTPUT_PREFIX}/jobs/{job_name}"
        
        try:
            create_autopilot_job(
                target_col=target_col,
                input_s3_path=prepared_datasets[target_col]['s3_path'],
                output_s3_path=output_s3_path,
                job_name=job_name
            )
            
            job_info[target_col] = {
                'job_name': job_name,
                'output_path': output_s3_path,
                'status': 'InProgress'
            }
            
            # Small delay between job submissions
            time.sleep(2)
            
        except Exception as e:
            print(f"Failed to create job for {target_col}: {str(e)}")
            job_info[target_col] = {
                'job_name': None,
                'status': 'Failed',
                'error': str(e)
            }
    
    # Step 4: Monitor all jobs
    print(f"\n[Step 4] Monitoring Autopilot jobs...")
    print(f"Launched {len([j for j in job_info.values() if j.get('job_name')])} jobs")
    print(f"\nNote: Jobs will run in parallel. This may take 1-3 hours depending on data size.")
    print(f"You can close this notebook - jobs will continue running.")
    
    results = {}
    for target_col, info in job_info.items():
        if info.get('job_name'):
            try:
                response = wait_for_autopilot_job(info['job_name'])
                results[target_col] = {
                    'job_name': info['job_name'],
                    'status': response['AutoMLJobStatus'],
                    'model_info': get_best_model_info(info['job_name'])
                }
            except Exception as e:
                print(f"Error monitoring {target_col}: {str(e)}")
                results[target_col] = {
                    'job_name': info['job_name'],
                    'status': 'Error',
                    'error': str(e)
                }
    
    # Step 5: Summary
    print(f"\n{'='*80}")
    print("PIPELINE EXECUTION SUMMARY")
    print(f"{'='*80}\n")
    
    for target_col, result in results.items():
        print(f"Target: {target_col}")
        print(f"  Job Name: {result['job_name']}")
        print(f"  Status: {result['status']}")
        
        if result.get('model_info'):
            info = result['model_info']
            print(f"  Best {info['objective_metric_name']}: {info['objective_metric_value']:.4f}")
            print(f"  Best Candidate: {info['candidate_name']}")
        print()
    
    # Save results to JSON
    results_file = f"autopilot_results_{timestamp}.json"
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    print(f"Results saved to: {results_file}")
    
    # Upload results to S3
    results_s3_path = f"{OUTPUT_PREFIX}/results/{results_file}"
    s3_client.upload_file(results_file, BUCKET_NAME, results_s3_path)
    print(f"Results uploaded to: s3://{BUCKET_NAME}/{results_s3_path}")
    
    return results


# ============================================================================
# EXECUTE PIPELINE
# ============================================================================

if __name__ == "__main__":
    # Run the complete pipeline
    results = run_autopilot_pipeline()
    
    print("\n" + "="*80)
    print("Pipeline execution completed!")
    print("="*80)
    print("\nNext steps:")
    print("1. Review model performance for each target in SageMaker Studio")
    print("2. Deploy the best models to endpoints for inference")
    print("3. Use batch transform for bulk predictions")
    print("\nYou can find all artifacts in S3:")
    print(f"  s3://{BUCKET_NAME}/{OUTPUT_PREFIX}/")