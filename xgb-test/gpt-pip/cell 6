import os
import json
import sagemaker
import boto3

# ---------------- Correct Imports ---------------- #
from sagemaker.workflow.pipeline import Pipeline
from sagemaker.workflow.steps import ProcessingStep, TrainingStep
from sagemaker.workflow.step_collections import RegisterModel

from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput
from sagemaker.inputs import TrainingInput
from sagemaker.workflow.functions import Join

from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo
from sagemaker.workflow.condition_step import ConditionStep

from sagemaker.xgboost.estimator import XGBoost
from sagemaker.workflow.pipeline_context import PipelineSession

# NEW: Import for ModelStep approach
from sagemaker.workflow.model_step import ModelStep
from sagemaker.model import Model

# -------------------------------------------------- #

region = boto3.Session().region_name
sm_sess = sagemaker.Session()          # regular session
pipeline_session = PipelineSession()   # MUST be used for all pipeline-bound jobs
role = sagemaker.get_execution_role()
BUCKET = sm_sess.default_bucket()

PROJECT_PREFIX = "multi-target-xgb"

TARGET_COLS = ["DepartmentCode", "AccountCode", "SubAccountCode", "LocationCode"]
INPUT_FEATURES = ["VendorName", "LineDescription", "ClubNumber"]

# INPUT CSV (already uploaded)
INPUT_S3 = f"s3://{BUCKET}/{PROJECT_PREFIX}/input/data.csv"

# ------------------------------------
# PREPROCESS STEP
# ------------------------------------
processor = ScriptProcessor(
    image_uri=sagemaker.image_uris.retrieve("sklearn", region, version="1.2-1"),
    role=role,
    command=["python3"],
    instance_type="ml.m5.xlarge",
    instance_count=1,
    sagemaker_session=pipeline_session,   # IMPORTANT
)

preprocess_output_prefix = f"{PROJECT_PREFIX}/processing"

step_preprocess = ProcessingStep(
    name="PreprocessData",
    processor=processor,
    inputs=[
        ProcessingInput(
            input_name="raw_data",
            source=INPUT_S3,
            destination="/opt/ml/processing/input",
        )
    ],
    outputs=[
        ProcessingOutput(
            output_name="proc_output",
            source="/opt/ml/processing/output",
            destination=f"s3://{BUCKET}/{preprocess_output_prefix}",
        )
    ],
    code="preprocess.py",
    job_arguments=[
        "--input_dir",
        "/opt/ml/processing/input",
        "--targets",
        json.dumps(TARGET_COLS),
        "--input_features",
        json.dumps(INPUT_FEATURES),
        "--output_dir",
        "/opt/ml/processing/output",
        "--tfidf_max_features",
        "2000",
    ],
)

proc_uri = step_preprocess.properties.ProcessingOutputConfig.Outputs[
    "proc_output"
].S3Output.S3Uri

# ------------------------------------
# TRAINING STEPS (4 models)
# ------------------------------------
train_steps = {}
models_output_prefix = f"{PROJECT_PREFIX}/trained-models"

for tgt in TARGET_COLS:

    estimator = XGBoost(
        entry_point="train.py",
        framework_version="1.5-1",
        instance_type="ml.m5.xlarge",
        instance_count=1,
        role=role,
        output_path=f"s3://{BUCKET}/{models_output_prefix}/{tgt}",
        base_job_name=f"xgb-{tgt}",
        sagemaker_session=pipeline_session,   # IMPORTANT
    )

    train_path = Join(on="/", values=[proc_uri, tgt, "train.libsvm"])
    val_path = Join(on="/", values=[proc_uri, tgt, "val.libsvm"])

    step_train = TrainingStep(
        name=f"Train_{tgt}",
        estimator=estimator,
        inputs={
            "train": TrainingInput(
                s3_data=train_path, content_type="text/libsvm"
            ),
            "validation": TrainingInput(
                s3_data=val_path, content_type="text/libsvm"
            ),
        },
    )

    train_steps[tgt] = step_train

# ------------------------------------
# REPACK STEP
# ------------------------------------
mme_output_prefix = f"{PROJECT_PREFIX}/mme-packaged"

repack_processor = ScriptProcessor(
    image_uri=sagemaker.image_uris.retrieve("sklearn", region, version="1.2-1"),
    role=role,
    command=["python3"],
    instance_type="ml.m5.xlarge",
    instance_count=1,
    sagemaker_session=pipeline_session,   # IMPORTANT
)

step_repack = ProcessingStep(
    name="RepackModelsForMME",
    processor=repack_processor,
    inputs=[
        ProcessingInput(
            input_name="models_dir",
            source=f"s3://{BUCKET}/{models_output_prefix}",
            destination="/opt/ml/processing/models",
        ),
        ProcessingInput(
            input_name="preprocess_dir",
            source=proc_uri,
            destination="/opt/ml/processing/preprocess",
        ),
        ProcessingInput(
            input_name="inference_script",
            source="inference.py",
            destination="/opt/ml/processing/code",
        ),
    ],
    outputs=[
        ProcessingOutput(
            output_name="mme_models",
            source="/opt/ml/processing/output",
            destination=f"s3://{BUCKET}/{mme_output_prefix}",
        )
    ],
    code="repack_for_mme.py",
    depends_on=list(train_steps.values()),
)

# ------------------------------------
# COMBINE MODELS STEP
# ------------------------------------
combine_processor = ScriptProcessor(
    image_uri=sagemaker.image_uris.retrieve("sklearn", region, version="1.2-1"),
    role=role,
    command=["python3"],
    instance_type="ml.m5.xlarge",
    instance_count=1,
    sagemaker_session=pipeline_session,   # IMPORTANT
)

combined_prefix = f"{PROJECT_PREFIX}/combined"

step_combine = ProcessingStep(
    name="CombineModels",
    processor=combine_processor,
    inputs=[
        ProcessingInput(
            input_name="mme_input",
            source=f"s3://{BUCKET}/{mme_output_prefix}",
            destination="/opt/ml/processing/mme",
        )
    ],
    outputs=[
        ProcessingOutput(
            output_name="combined",
            source="/opt/ml/processing/output",
            destination=f"s3://{BUCKET}/{combined_prefix}",
        )
    ],
    code="combine_models.py",
    job_arguments=[
        "--mme_input",
        f"s3://{BUCKET}/{mme_output_prefix}",
        "--bucket",
        BUCKET,
        "--combined_prefix",
        combined_prefix,
        "--output_dir",
        "/opt/ml/processing/output",
    ],
)

combined_model_uri = step_combine.properties.ProcessingOutputConfig.Outputs[
    "combined"
].S3Output.S3Uri

# ------------------------------------
# EVALUATION STEP
# ------------------------------------
eval_processor = ScriptProcessor(
    image_uri=sagemaker.image_uris.retrieve("xgboost", region, version="1.5-1"),
    role=role,
    command=["python3"],
    instance_type="ml.m5.xlarge",
    instance_count=1,
    sagemaker_session=pipeline_session,   # IMPORTANT
)

eval_prefix = f"{PROJECT_PREFIX}/evaluation"

step_evaluate = ProcessingStep(
    name="EvaluateModel",
    processor=eval_processor,
    inputs=[
        ProcessingInput(
            input_name="preprocess_dir",
            source=proc_uri,
            destination="/opt/ml/processing/pre",
        ),
        ProcessingInput(
            input_name="models_dir",
            source=f"s3://{BUCKET}/{models_output_prefix}",
            destination="/opt/ml/processing/models",
        ),
    ],
    outputs=[
        ProcessingOutput(
            output_name="evaluation",
            source="/opt/ml/processing/output",
            destination=f"s3://{BUCKET}/{eval_prefix}",
        )
    ],
    code="evaluate.py",
    job_arguments=[
        "--preprocess_dir",
        "/opt/ml/processing/pre",
        "--models_dir",
        "/opt/ml/processing/models",
        "--targets",
        json.dumps(TARGET_COLS),
        "--output_dir",
        "/opt/ml/processing/output",
    ],
)

# ------------------------------------
# REGISTER MODEL (combined MME tar in custom image)
# ------------------------------------
step_register = RegisterModel(
    name="RegisterCustomModel",
    model_data=combined_model_uri,
    image_uri="<YOUR_ECR_CUSTOM_IMAGE>",
    content_types=["application/json"],
    response_types=["application/json"],
    inference_instances=["ml.m5.xlarge"],   # you can adjust
    transform_instances=["ml.m5.xlarge"],
    model_package_group_name="MultiTargetXGB-Custom",
    approval_status="PendingManualApproval",
    execution_role_arn=role,
)

# ------------------------------------
# CONDITION STEP (placeholder threshold)
# ------------------------------------
cond = ConditionGreaterThanOrEqualTo(
    left=0.15,  # TODO: wire from evaluation.json via JsonGet
    right=0.10,
)

# ------------------------------------
# MODEL DEPLOYMENT STEPS (REPLACED LAMBDA WITH MODELSTEP)
# ------------------------------------

# Create a Model object for deployment
custom_model = Model(
    image_uri="<YOUR_ECR_CUSTOM_IMAGE>",  # Replace with your actual ECR image URI
    model_data=combined_model_uri,
    role=role,
    sagemaker_session=pipeline_session,
)

# ModelStep to create the model
step_create_model = ModelStep(
    name="CreateModel",
    step_args=custom_model.create(
        instance_type="ml.m5.xlarge",
    ),
)

# Note: For endpoint creation/update, you have two options:
# Option A: Handle endpoint deployment outside the pipeline (manually or via separate script)
# Option B: Use a custom processing step that calls boto3 to create/update endpoint

# OPTION B: Processing step for endpoint deployment
endpoint_processor = ScriptProcessor(
    image_uri=sagemaker.image_uris.retrieve("sklearn", region, version="1.2-1"),
    role=role,
    command=["python3"],
    instance_type="ml.m5.xlarge",
    instance_count=1,
    sagemaker_session=pipeline_session,
)

# You'll need to create this script
step_deploy_endpoint = ProcessingStep(
    name="DeployEndpoint",
    processor=endpoint_processor,
    code="deploy_endpoint.py",  # Create this script (shown below)
    job_arguments=[
        "--model-name", custom_model.name,
        "--endpoint-name", "multi-target-xgb-custom-endpoint",
        "--endpoint-config", "multi-target-xgb-config",
        "--instance-type", "ml.m5.xlarge",
        "--region", region,
        "--role", role,
    ],
)

# ------------------------------------
# CONDITION: Only deploy if model is good
# ------------------------------------
step_condition = ConditionStep(
    name="CheckAccuracy",
    conditions=[cond],
    if_steps=[step_create_model, step_deploy_endpoint],
    else_steps=[],
)

# ------------------------------------
# FINAL PIPELINE
# ------------------------------------
pipeline = Pipeline(
    name="MultiTargetXGB-CustomContainer-Pipeline",
    steps=[
        step_preprocess,
        *train_steps.values(),
        step_repack,
        step_combine,
        step_evaluate,
        step_register,
        step_condition,
    ],
    sagemaker_session=pipeline_session,
)

pipeline.upsert(role_arn=role)
print("PIPELINE READY.")