# ============================
# Cell 6 â€” Preprocess + Train + Repack + Combine + Evaluate + Deploy
# ============================
import os
import json
import sagemaker

from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput
from sagemaker.workflow.steps import ProcessingStep, TrainingStep
from sagemaker.xgboost.estimator import XGBoost
from sagemaker.inputs import TrainingInput
from sagemaker.workflow.pipeline_context import PipelineSession
from sagemaker.workflow.pipeline import Pipeline
from sagemaker.workflow.functions import Join

# Reuse region / role / BUCKET / PROJECT_PREFIX / TARGET_COLS / INPUT_FEATURES / INPUT_S3

pipeline_session = PipelineSession()

# -------------------------------
# PREPROCESSING STEP
# -------------------------------
processor = ScriptProcessor(
    image_uri=sagemaker.image_uris.retrieve("sklearn", region, version="1.2-1"),
    role=role,
    command=["python3"],
    instance_type="ml.m5.xlarge",
    instance_count=1,
)

preprocess_output = (
    f"{PROJECT_NAME}/processing"
    if "PROJECT_NAME" in globals()
    else f"{PROJECT_PREFIX}/processing"
)

step_preprocess = ProcessingStep(
    name="PreprocessData",
    processor=processor,
    inputs=[
        ProcessingInput(
            input_name="raw_data",
            source=INPUT_S3,
            destination="/opt/ml/processing/input",
        )
    ],
    outputs=[
        ProcessingOutput(
            output_name="proc_output",
            source="/opt/ml/processing/output",
            destination=f"s3://{BUCKET}/{preprocess_output}",
        )
    ],
    code="preprocess.py",
    job_arguments=[
        "--input_dir",
        "/opt/ml/processing/input",
        "--targets",
        json.dumps(TARGET_COLS),
        "--input_features",
        json.dumps(INPUT_FEATURES),
        "--output_dir",
        "/opt/ml/processing/output",
        "--tfidf_max_features",
        "2000",
    ],
)

proc_uri = step_preprocess.properties.ProcessingOutputConfig.Outputs[
    "proc_output"
].S3Output.S3Uri

# -------------------------------
# TRAINING STEPS
# -------------------------------
models_output = (
    f"{PROJECT_NAME}/trained-models"
    if "PROJECT_NAME" in globals()
    else f"{PROJECT_PREFIX}/trained-models"
)

train_steps = {}

for tgt in TARGET_COLS:
    estimator = XGBoost(
        entry_point="train.py",
        framework_version="1.5-1",
        instance_type="ml.m5.xlarge",
        instance_count=1,
        role=role,
        output_path=f"s3://{BUCKET}/{models_output}/{tgt}",
        base_job_name=f"xgb-{tgt}",
    )

    train_path = Join(on="/", values=[proc_uri, tgt, "train.libsvm"])
    val_path   = Join(on="/", values=[proc_uri, tgt, "val.libsvm"])

    step_train = TrainingStep(
        name=f"Train_{tgt}",
        estimator=estimator,
        inputs={
            "train": TrainingInput(
                s3_data=train_path, content_type="text/libsvm"
            ),
            "validation": TrainingInput(
                s3_data=val_path, content_type="text/libsvm"
            ),
        },
    )

    train_steps[tgt] = step_train

# -------------------------------
# REPACK STEP
# -------------------------------
mme_output = (
    f"{PROJECT_NAME}/mme-packaged"
    if "PROJECT_NAME" in globals()
    else f"{PROJECT_PREFIX}/mme-packaged"
)

repack_processor = ScriptProcessor(
    image_uri=sagemaker.image_uris.retrieve("sklearn", region, version="1.2-1"),
    role=role,
    command=["python3"],
    instance_type="ml.m5.xlarge",
    instance_count=1,
)

step_repack = ProcessingStep(
    name="RepackModelsForMME",
    processor=repack_processor,
    inputs=[
        ProcessingInput(
            input_name="models_dir",
            source=f"s3://{BUCKET}/{models_output}",
            destination="/opt/ml/processing/models",
        ),
        ProcessingInput(
            input_name="preprocess_dir",
            source=proc_uri,
            destination="/opt/ml/processing/preprocess",
        ),
        ProcessingInput(
            input_name="inference_script",
            source="inference.py",
            destination="/opt/ml/processing/code",
        ),
    ],
    outputs=[
        ProcessingOutput(
            output_name="mme_models",
            source="/opt/ml/processing/output",
            destination=f"s3://{BUCKET}/{mme_output}",
        )
    ],
    code="repack_for_mme.py",
    job_arguments=[
        "--models_dir",
        "/opt/ml/processing/models",
        "--preprocess_dir",
        "/opt/ml/processing/preprocess",
        "--output_dir",
        "/opt/ml/processing/output",
    ],
    depends_on=list(train_steps.values()),
)

# -------------------------------
# COMBINE MODELS STEP
# -------------------------------
combined_output = (
    f"{PROJECT_NAME}/combined"
    if "PROJECT_NAME" in globals()
    else f"{PROJECT_PREFIX}/combined"
)

combine_processor = ScriptProcessor(
    image_uri=sagemaker.image_uris.retrieve("sklearn", region, version="1.2-1"),
    role=role,
    command=["python3"],
    instance_type="ml.m5.xlarge",
    instance_count=1,
)

step_combine = ProcessingStep(
    name="CombineModels",
    processor=combine_processor,
    inputs=[
        ProcessingInput(
            input_name="mme_input",
            source=f"s3://{BUCKET}/{mme_output}",
            destination="/opt/ml/processing/mme",
        )
    ],
    outputs=[
        ProcessingOutput(
            output_name="combined",
            source="/opt/ml/processing/output",
            destination=f"s3://{BUCKET}/{combined_output}",
        )
    ],
    code="combine_models.py",
    job_arguments=[
        "--mme_input",
        mme_output,
        "--bucket",
        BUCKET,
        "--combined_prefix",
        combined_output,
        "--output_dir",
        "/opt/ml/processing/output",
    ],
    depends_on=[step_repack],
)

# Pipeline property for combined prefix S3 URI
combined_prefix_uri = step_combine.properties.ProcessingOutputConfig.Outputs[
    "combined"
].S3Output.S3Uri

# Full S3 URI to model.tar.gz
combined_model_uri = Join(
    on="/",
    values=[combined_prefix_uri, "model.tar.gz"],
)

# -------------------------------
# EVALUATION STEP
# -------------------------------
eval_output = (
    f"{PROJECT_NAME}/evaluation"
    if "PROJECT_NAME" in globals()
    else f"{PROJECT_PREFIX}/evaluation"
)

eval_processor = ScriptProcessor(
    image_uri=sagemaker.image_uris.retrieve("xgboost", region, version="1.5-1"),
    role=role,
    command=["python3"],
    instance_type="ml.m5.xlarge",
    instance_count=1,
)

step_evaluate = ProcessingStep(
    name="EvaluateModel",
    processor=eval_processor,
    inputs=[
        ProcessingInput(
            input_name="preprocess_dir",
            source=proc_uri,
            destination="/opt/ml/processing/pre",
        ),
        ProcessingInput(
            input_name="models_dir",
            source=f"s3://{BUCKET}/{models_output}",
            destination="/opt/ml/processing/models",
        ),
    ],
    outputs=[
        ProcessingOutput(
            output_name="evaluation",
            source="/opt/ml/processing/output",
            destination=f"s3://{BUCKET}/{eval_output}",
        )
    ],
    code="evaluate.py",
    job_arguments=[
        "--preprocess_dir",
        "/opt/ml/processing/pre",
        "--models_dir",
        "/opt/ml/processing/models",
        "--targets",
        json.dumps(TARGET_COLS),
        "--output_dir",
        "/opt/ml/processing/output",
    ],
    depends_on=list(train_steps.values()),
)

# -------------------------------
# NEW: DEPLOYMENT STEP (ProcessingStep)
# -------------------------------
MODEL_NAME = f"{PROJECT_PREFIX}-custom-model"
ENDPOINT_CONFIG_NAME = f"{PROJECT_PREFIX}-custom-config"
ENDPOINT_NAME = f"{PROJECT_PREFIX}-custom-endpoint"
INSTANCE_TYPE = "ml.m5.xlarge"
INSTANCE_COUNT = 1
CUSTOM_IMAGE_URI = "<YOUR_ECR_CUSTOM_IMAGE>"  # TODO: replace

deploy_processor = ScriptProcessor(
    image_uri=sagemaker.image_uris.retrieve("sklearn", region, version="1.2-1"),
    role=role,
    command=["python3"],
    instance_type="ml.m5.xlarge",
    instance_count=1,
)

step_deploy = ProcessingStep(
    name="DeployEndpoint",
    processor=deploy_processor,
    code="deploy_endpoint.py",
    inputs=[],   # no S3 inputs needed
    outputs=[],  # no outputs; just side-effect: model + endpoint
    job_arguments=[
        "--model_name", MODEL_NAME,
        "--endpoint_config_name", ENDPOINT_CONFIG_NAME,
        "--endpoint_name", ENDPOINT_NAME,
        "--image_uri", CUSTOM_IMAGE_URI,
        "--model_data", combined_model_uri,
        "--instance_type", INSTANCE_TYPE,
        "--instance_count", str(INSTANCE_COUNT),
    ],
    depends_on=[step_evaluate],  # ensure evaluation finishes before deploy
)

# -------------------------------
# PIPELINE DEFINITION
# -------------------------------
pipeline = Pipeline(
    name="MultiTargetXGBPipeline-LIBSVM-Combined-Eval-Deploy",
    steps=(
        [step_preprocess]
        + list(train_steps.values())
        + [step_repack, step_combine, step_evaluate, step_deploy]
    ),
    sagemaker_session=pipeline_session,
)

pipeline.upsert(role_arn=role)
print("ðŸŽ‰ PIPELINE WITH DEPLOYMENT STEP CREATED SUCCESSFULLY!")
