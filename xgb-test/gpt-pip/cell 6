# ============================
# Simplified Pipeline
# preprocess -> train (per target) -> combine -> deploy
# ============================
import os
import json
import sagemaker
from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput
from sagemaker.workflow.steps import ProcessingStep, TrainingStep
from sagemaker.xgboost.estimator import XGBoost
from sagemaker.inputs import TrainingInput
from sagemaker.workflow.pipeline_context import PipelineSession
from sagemaker.workflow.pipeline import Pipeline
from sagemaker.workflow.functions import Join

# Sessions
sm_session = sm_session if "sm_session" in globals() else sagemaker.Session()
pipeline_session = PipelineSession(sagemaker_session=sm_session)

# Reuse region/role/BUCKET/PROJECT_PREFIX/INPUT_S3/TARGET_COLS/INPUT_FEATURES

# -------------------------------
# PREPROCESS
# -------------------------------
preprocess_processor = ScriptProcessor(
    image_uri=sagemaker.image_uris.retrieve("sklearn", region, version="1.2-1"),
    role=role,
    command=["python3"],
    instance_type="ml.m5.xlarge",
    instance_count=1,
    sagemaker_session=pipeline_session,
)

preprocess_prefix = f"{PROJECT_PREFIX}/processing"

step_preprocess = ProcessingStep(
    name="PreprocessData",
    processor=preprocess_processor,
    inputs=[
        ProcessingInput(
            input_name="raw_data",
            source=INPUT_S3,
            destination="/opt/ml/processing/input",
        )
    ],
    outputs=[
        ProcessingOutput(
            output_name="proc_output",
            source="/opt/ml/processing/output",
            destination=f"s3://{BUCKET}/{preprocess_prefix}",
        )
    ],
    code="preprocess.py",
    job_arguments=[
        "--input_dir", "/opt/ml/processing/input",
        "--targets", json.dumps(TARGET_COLS),
        "--input_features", json.dumps(INPUT_FEATURES),
        "--output_dir", "/opt/ml/processing/output",
        "--hash_dim", "2000",
        "--test_size", "0.2",
        "--random_state", "42",
    ],
)

proc_uri = step_preprocess.properties.ProcessingOutputConfig.Outputs["proc_output"].S3Output.S3Uri

# -------------------------------
# TRAIN (per target)
# -------------------------------
models_prefix = f"{PROJECT_PREFIX}/trained-models"
train_steps = {}

for tgt in TARGET_COLS:
    estimator = XGBoost(
        entry_point="train.py",
        framework_version="1.5-1",
        instance_type="ml.m5.xlarge",
        instance_count=1,
        role=role,
        output_path=f"s3://{BUCKET}/{models_prefix}/{tgt}",
        base_job_name=f"xgb-{tgt}",
        sagemaker_session=pipeline_session,
    )

    train_path = Join(on="/", values=[proc_uri, tgt, "train.libsvm"])
    val_path   = Join(on="/", values=[proc_uri, tgt, "val.libsvm"])

    step_train = TrainingStep(
        name=f"Train_{tgt}",
        estimator=estimator,
        inputs={
            "train": TrainingInput(s3_data=train_path, content_type="text/libsvm"),
            "validation": TrainingInput(s3_data=val_path, content_type="text/libsvm"),
        },
    )
    train_steps[tgt] = step_train

# -------------------------------
# COMBINE (build final model.tar.gz for your custom container)
# -------------------------------
combine_processor = ScriptProcessor(
    image_uri=sagemaker.image_uris.retrieve("sklearn", region, version="1.2-1"),
    role=role,
    command=["python3"],
    instance_type="ml.m5.xlarge",
    instance_count=1,
    sagemaker_session=pipeline_session,
)

combined_prefix = f"{PROJECT_PREFIX}/combined"

step_combine = ProcessingStep(
    name="CombineModels",
    processor=combine_processor,
    inputs=[
        # Trained model artifacts (S3) mounted locally
        ProcessingInput(
            input_name="models_in",
            source=f"s3://{BUCKET}/{models_prefix}",
            destination="/opt/ml/processing/models_in",
        ),
        # Preprocess artifacts (S3) mounted locally
        ProcessingInput(
            input_name="preprocess_in",
            source=proc_uri,
            destination="/opt/ml/processing/preprocess",
        ),
    ],
    outputs=[
        ProcessingOutput(
            output_name="combined",
            source="/opt/ml/processing/output",
            destination=f"s3://{BUCKET}/{combined_prefix}",
        )
    ],
    code="combine_models.py",
    job_arguments=[
        "--models_in_dir", "/opt/ml/processing/models_in",
        "--preprocess_dir", "/opt/ml/processing/preprocess",
        "--output_dir", "/opt/ml/processing/output",
        "--targets", ",".join(TARGET_COLS),
    ],
    depends_on=list(train_steps.values()),
)

combined_s3_uri = step_combine.properties.ProcessingOutputConfig.Outputs["combined"].S3Output.S3Uri
combined_model_uri = Join(on="/", values=[combined_s3_uri, "model.tar.gz"])

# -------------------------------
# DEPLOY (uses your existing custom container)
# -------------------------------
MODEL_NAME = f"{PROJECT_PREFIX}-custom-model"
ENDPOINT_CONFIG_NAME = f"{PROJECT_PREFIX}-custom-config"
ENDPOINT_NAME = f"{PROJECT_PREFIX}-custom-endpoint"
INSTANCE_TYPE = "ml.m5.xlarge"
INSTANCE_COUNT = 1
CUSTOM_IMAGE_URI = "<YOUR_ECR_CUSTOM_IMAGE>"  # replace

deploy_processor = ScriptProcessor(
    image_uri=sagemaker.image_uris.retrieve("sklearn", region, version="1.2-1"),
    role=role,
    command=["python3"],
    instance_type="ml.m5.xlarge",
    instance_count=1,
    sagemaker_session=pipeline_session,
)

step_deploy = ProcessingStep(
    name="DeployEndpoint",
    processor=deploy_processor,
    code="deploy_endpoint.py",
    inputs=[],
    outputs=[],
    job_arguments=[
        "--model_name", MODEL_NAME,
        "--endpoint_config_name", ENDPOINT_CONFIG_NAME,
        "--endpoint_name", ENDPOINT_NAME,
        "--image_uri", CUSTOM_IMAGE_URI,
        "--model_data", combined_model_uri,
        "--instance_type", INSTANCE_TYPE,
        "--instance_count", str(INSTANCE_COUNT),
        "--role_arn", role,
    ],
    depends_on=[step_combine],
)

# -------------------------------
# PIPELINE
# -------------------------------
pipeline = Pipeline(
    name="MultiTargetXGBPipeline-CustomContainer-Combined",
    steps=[step_preprocess] + list(train_steps.values()) + [step_combine, step_deploy],
    sagemaker_session=pipeline_session,
)

pipeline.upsert(role_arn=role)
print("âœ… Simplified pipeline upserted")
