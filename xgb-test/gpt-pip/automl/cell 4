import json
import numpy as np
import pandas as pd
import boto3
import io

runtime = boto3.client("sagemaker-runtime", region_name=region)
s3 = boto3.client("s3", region_name=region)

endpoint_name = f"{PROJECT_PREFIX}-custom-endpoint"

def list_s3_keys(prefix_s3_uri: str):
    b, p = parse_s3_uri(prefix_s3_uri)
    paginator = s3.get_paginator("list_objects_v2")
    keys = []
    for page in paginator.paginate(Bucket=b, Prefix=p):
        for obj in page.get("Contents", []):
            k = obj["Key"]
            # SageMaker batch transform outputs usually end with .out
            if k.endswith(".out") or k.endswith(".csv") or k.endswith(".txt"):
                keys.append((b, k))
    return keys

def read_s3_text(bucket, key) -> str:
    return s3.get_object(Bucket=bucket, Key=key)["Body"].read().decode("utf-8")

def normalize_series(s: pd.Series) -> pd.Series:
    return s.astype(str).str.strip()

def autopilot_parse_predictions(text: str) -> list:
    """
    Tries to parse batch transform outputs.
    Common patterns:
      - single column: predicted label
      - multiple columns: label + probs
      - JSON lines (rare)
    Returns list of predicted labels (as strings).
    """
    text = text.strip()
    if not text:
        return []

    # JSONL fallback
    if text.startswith("{") and "\n" in text:
        preds = []
        for line in text.splitlines():
            try:
                obj = json.loads(line)
                # try common keys
                for k in ("predicted_label", "prediction", "label", "pred"):
                    if k in obj:
                        preds.append(str(obj[k]))
                        break
                else:
                    preds.append(str(obj))
            except Exception:
                pass
        if preds:
            return preds

    # CSV fallback
    dfp = pd.read_csv(io.StringIO(text), header=None)
    # If first column looks like label, use it.
    return dfp.iloc[:, 0].astype(str).tolist()

comparison = {}

for tgt in autopilot_paths.keys():
    # Load the same validation set used for Autopilot
    val_df = pd.read_csv(autopilot_paths[tgt]["val"])
    gt = normalize_series(val_df[tgt])

    # --- XGB endpoint predictions ---
    instances = val_df[INPUT_FEATURES].to_dict(orient="records")
    payload = {"target": tgt, "instances": instances}

    resp = runtime.invoke_endpoint(
        EndpointName=endpoint_name,
        ContentType="application/json",
        Body=json.dumps(payload),
    )
    resp_json = json.loads(resp["Body"].read().decode("utf-8"))
    xgb_preds = [p["prediction"] for p in resp_json["predictions"]]
    xgb_preds = pd.Series(xgb_preds)
    xgb_acc = float((normalize_series(xgb_preds) == gt).mean())

    # --- Autopilot predictions (batch transform outputs) ---
    ap_out_prefix = autopilot_pred_outputs.get(tgt)
    if not ap_out_prefix:
        print(f"[{tgt}] no autopilot preds, skipping comparison")
        continue

    out_keys = list_s3_keys(ap_out_prefix)
    if not out_keys:
        raise RuntimeError(f"[{tgt}] no batch transform output files found under {ap_out_prefix}")

    ap_pred_list = []
    for b, k in out_keys:
        ap_pred_list.extend(autopilot_parse_predictions(read_s3_text(b, k)))

    ap_preds = pd.Series(ap_pred_list)

    # Batch transform sometimes writes headers or different row counts depending on configs;
    # align on min length defensively.
    n = min(len(gt), len(ap_preds))
    ap_acc = float((normalize_series(ap_preds.iloc[:n]) == gt.iloc[:n]).mean())

    comparison[tgt] = {
        "rows_val": int(len(gt)),
        "xgb_acc": xgb_acc,
        "autopilot_acc": ap_acc,
        "note": f"autopilot_preds_rows={len(ap_preds)} (aligned to {n})",
    }

print("\n=== Accuracy comparison (XGB endpoint vs Autopilot) ===")
for tgt, m in comparison.items():
    print(
        f"{tgt}: XGB={m['xgb_acc']:.4f} | Autopilot={m['autopilot_acc']:.4f} "
        f"| val_rows={m['rows_val']} | {m['note']}"
    )
