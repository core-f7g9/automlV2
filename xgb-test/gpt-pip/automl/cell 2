from sklearn.feature_extraction.text import HashingVectorizer
from scipy import sparse
import xgboost as xgb

# ----- Feature construction -----
hash_dim = 2000
vec = HashingVectorizer(
    n_features=hash_dim,
    alternate_sign=False,
    norm=None
)

X_train_text = vec.transform(
    (train_df["VendorName"].fillna("") + " " +
     train_df["LineDescription"].fillna("")).astype(str)
)

X_val_text = vec.transform(
    (val_df["VendorName"].fillna("") + " " +
     val_df["LineDescription"].fillna("")).astype(str)
)

X_train_num = sparse.csr_matrix(train_df[["ClubNumber"]].fillna(0).values)
X_val_num   = sparse.csr_matrix(val_df[["ClubNumber"]].fillna(0).values)

X_train = sparse.hstack([X_train_text, X_train_num])
X_val   = sparse.hstack([X_val_text, X_val_num])

# ----- Labels -----
labels = sorted(train_df[TARGET].unique())
label_map = {v: i for i, v in enumerate(labels)}

y_train = train_df[TARGET].map(label_map).values
y_val   = val_df[TARGET].map(label_map).values

# ----- Train XGBoost -----
dtrain = xgb.DMatrix(X_train, label=y_train)
dval   = xgb.DMatrix(X_val, label=y_val)

params = {
    "objective": "multi:softprob",
    "num_class": len(label_map),
    "eval_metric": "merror",
    "tree_method": "hist",
    "max_depth": 8,
    "eta": 0.2,
}

bst = xgb.train(
    params,
    dtrain,
    num_boost_round=200,
    evals=[(dval, "val")],
    verbose_eval=False
)

# ----- Evaluate -----
preds = bst.predict(dval)
xgb_acc = (preds.argmax(axis=1) == y_val).mean()

print(f"XGBoost accuracy: {xgb_acc:.4f}")
