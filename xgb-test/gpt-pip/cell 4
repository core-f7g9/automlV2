%%writefile repack_for_mme.py
import argparse
import os
import tarfile
import shutil
import glob
import logging
import traceback

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def list_dir(root, label):
    """Log a tree of a directory."""
    if not os.path.exists(root):
        logger.info("[%s] %s does not exist", label, root)
        return
    for cur, dirs, files in os.walk(root):
        rel = os.path.relpath(cur, root)
        logger.info("[%s] %s dirs=%s files=%s", label, rel, dirs, files)


def safe_extract_tar(path, dest):
    if not os.path.exists(path):
        logger.error("[safe_extract_tar] Missing tar: %s", path)
        return False
    try:
        with tarfile.open(path, "r:gz") as tar:
            members = [m.name for m in tar.getmembers()]
            logger.info("[safe_extract_tar] Extracting %s with members=%s", path, members)
            tar.extractall(dest)
        return True
    except Exception as e:
        logger.error("[safe_extract_tar] Failed to extract %s: %s", path, e, exc_info=True)
        return False


def find_model_tar(tgt_dir):
    """
    Look for model.tar.gz in common SageMaker locations.
    """
    candidates = [
        os.path.join(tgt_dir, "model.tar.gz"),
        os.path.join(tgt_dir, "output", "model.tar.gz"),
    ]
    candidates.extend(
        glob.glob(os.path.join(tgt_dir, "**", "model.tar.gz"), recursive=True)
    )
    for c in candidates:
        if os.path.exists(c):
            return c
    return None


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--models_dir")
    parser.add_argument("--preprocess_dir")
    parser.add_argument("--output_dir")
    args = parser.parse_args()

    logger.info("=== repack_for_mme starting ===")
    logger.info("models_dir=%s", args.models_dir)
    logger.info("preprocess_dir=%s", args.preprocess_dir)
    logger.info("output_dir=%s", args.output_dir)

    os.makedirs(args.output_dir, exist_ok=True)

    list_dir(args.models_dir, "MODELS_DIR_BEFORE")
    list_dir(args.preprocess_dir, "PREPROCESS_DIR")

    # Shared preprocess artifacts
    preprocess_artifacts = glob.glob(os.path.join(args.preprocess_dir, "labelenc_*.pkl"))
    meta_file = os.path.join(args.preprocess_dir, "preprocess_meta.json")
    logger.info(
        "Found %d labelenc files, meta_file exists=%s",
        len(preprocess_artifacts),
        os.path.exists(meta_file),
    )

    packaged = 0

    for tgt in os.listdir(args.models_dir):
        tgt_dir = os.path.join(args.models_dir, tgt)
        if not os.path.isdir(tgt_dir):
            continue

        logger.info("------ Target %s ------", tgt)
        list_dir(tgt_dir, f"TARGET_{tgt}_BEFORE")

        model_tar = find_model_tar(tgt_dir)
        if not model_tar:
            logger.error("[Target %s] No model.tar.gz found under %s", tgt, tgt_dir)
            continue
        logger.info("[Target %s] Using model_tar=%s", tgt, model_tar)

        # Extract original model.tar.gz
        extract_dir = os.path.join(tgt_dir, "extract_for_mme")
        if os.path.exists(extract_dir):
            shutil.rmtree(extract_dir)
        os.makedirs(extract_dir, exist_ok=True)

        if not safe_extract_tar(model_tar, extract_dir):
            logger.error("[Target %s] Failed to extract %s", tgt, model_tar)
            continue

        list_dir(extract_dir, f"TARGET_{tgt}_AFTER_EXTRACT")

        xgb_model = os.path.join(extract_dir, "xgboost-model")
        class_map = os.path.join(extract_dir, "class_map.json")

        if not os.path.exists(xgb_model):
            logger.error(
                "[Target %s] xgboost-model missing in %s; contents=%s",
                tgt,
                extract_dir,
                os.listdir(extract_dir),
            )
            continue

        if not os.path.exists(class_map):
            logger.warning(
                "[Target %s] class_map.json missing in %s; predictions may fail",
                tgt,
                extract_dir,
            )

        # Build a clean package root from extracted model
        pkg_root = os.path.join(args.output_dir, tgt + "_pkg")
        if os.path.exists(pkg_root):
            shutil.rmtree(pkg_root)
        shutil.copytree(extract_dir, pkg_root)

        # Copy preprocess artifacts into package root (so model_dir sees them directly)
        if os.path.exists(meta_file):
            shutil.copy(meta_file, os.path.join(pkg_root, "preprocess_meta.json"))
        else:
            logger.warning("[Target %s] preprocess_meta.json not found", tgt)

        for enc in preprocess_artifacts:
            shutil.copy(enc, pkg_root)

        list_dir(pkg_root, f"TARGET_{tgt}_PKG_BEFORE_TAR")

        out_tar = os.path.join(args.output_dir, f"{tgt}.tar.gz")
        logger.info("[Target %s] Writing final tar %s", tgt, out_tar)

        # IMPORTANT: arcname="." so contents land directly under /opt/ml/model
        with tarfile.open(out_tar, "w:gz") as tar:
            tar.add(pkg_root, arcname=".")

        logger.info("[Target %s] Wrote tar successfully", tgt)
        packaged += 1

    logger.info("=== repack_for_mme finished; packaged=%d ===", packaged)


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logger.error("Fatal error in repack_for_mme: %s", e, exc_info=True)
        raise
