%%writefile repack_for_mme.py
import argparse
import os
import tarfile
import shutil
import glob
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def find_model_tar(tgt_dir):
    """Search for model.tar.gz in the target directory or subdirectories"""
    candidates = [
        os.path.join(tgt_dir, "model.tar.gz"),
        os.path.join(tgt_dir, "output", "model.tar.gz")
    ]
    candidates.extend(glob.glob(os.path.join(tgt_dir, "**", "model.tar.gz"), recursive=True))
    for c in candidates:
        if os.path.exists(c):
            return c
    return None

def safe_extract(path, dest):
    """Extract tar.gz file with error handling"""
    try:
        with tarfile.open(path, "r:gz") as tar:
            tar.extractall(dest)
        return True
    except Exception as e:
        logger.error("Extraction failed for %s: %s", path, e)
        return False

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--models_dir")
    parser.add_argument("--preprocess_dir")
    parser.add_argument("--output_dir")
    args = parser.parse_args()

    os.makedirs(args.output_dir, exist_ok=True)
    
    # Locate artifacts from the preprocessing step
    preprocess_artifacts = glob.glob(os.path.join(args.preprocess_dir, "labelenc_*.pkl"))
    meta_file = os.path.join(args.preprocess_dir, "preprocess_meta.json")
    
    # UPDATED: The inference script location
    inference_script_src = "/opt/ml/processing/code/inference.py"
    
    # Validate that inference script exists
    if not os.path.exists(inference_script_src):
        logger.error("inference.py not found at %s", inference_script_src)
        logger.error("Available files in /opt/ml/processing/code/:")
        if os.path.exists("/opt/ml/processing/code"):
            for f in os.listdir("/opt/ml/processing/code"):
                logger.error("  - %s", f)
        raise FileNotFoundError(f"inference.py not found at {inference_script_src}")
    
    # Validate preprocessing artifacts exist
    if not os.path.exists(meta_file):
        logger.error("preprocess_meta.json not found at %s", meta_file)
        raise FileNotFoundError(f"Missing {meta_file}")
    
    logger.info("Found %d label encoders", len(preprocess_artifacts))
    logger.info("Preprocessing meta file: %s", meta_file)
    logger.info("Inference script: %s", inference_script_src)

    # Iterate over every target folder (e.g. DepartmentCode, AccountCode...)
    for tgt in os.listdir(args.models_dir):
        tgt_dir = os.path.join(args.models_dir, tgt)
        if not os.path.isdir(tgt_dir):
            continue

        logger.info("="*60)
        logger.info("Processing target: %s", tgt)
        logger.info("="*60)
        
        model_tar = find_model_tar(tgt_dir)

        if not model_tar:
            logger.warning("No model.tar.gz found for %s", tgt)
            continue

        # Extract the original training output
        extract_dir = os.path.join(tgt_dir, "extract_tmp")
        if os.path.exists(extract_dir):
            shutil.rmtree(extract_dir)
        os.makedirs(extract_dir, exist_ok=True)

        if not safe_extract(model_tar, extract_dir):
            continue

        # Find the actual booster file (usually named 'xgboost-model')
        booster_path = os.path.join(extract_dir, "xgboost-model")
        if not os.path.exists(booster_path):
            logger.error("xgboost-model binary not found inside tar for %s", tgt)
            logger.error("Contents of extract_dir: %s", os.listdir(extract_dir))
            continue

        # Find class_map.json
        class_map_path = os.path.join(extract_dir, "class_map.json")
        if not os.path.exists(class_map_path):
            logger.error("class_map.json not found for %s", tgt)
            continue

        # ---------------------------------------------------------
        # BUILD PACKAGE: Standard MME Structure
        # ---------------------------------------------------------
        
        pkg_root = os.path.join(args.output_dir, f"{tgt}_pkg")
        if os.path.exists(pkg_root):
            shutil.rmtree(pkg_root)
        os.makedirs(pkg_root)
        
        code_dir = os.path.join(pkg_root, "code")
        os.makedirs(code_dir)

        # 1. Copy Booster to ROOT of package
        logger.info("  Copying xgboost-model to root...")
        shutil.copy(booster_path, os.path.join(pkg_root, "xgboost-model"))

        # 2. Copy class_map.json to CODE directory
        logger.info("  Copying class_map.json to code/...")
        shutil.copy(class_map_path, os.path.join(code_dir, "class_map.json"))

        # 3. Copy preprocessing metadata to CODE directory
        logger.info("  Copying preprocess_meta.json to code/...")
        shutil.copy(meta_file, os.path.join(code_dir, "preprocess_meta.json"))

        # 4. Copy label encoders to CODE directory
        logger.info("  Copying %d label encoders to code/...", len(preprocess_artifacts))
        for enc in preprocess_artifacts:
            shutil.copy(enc, code_dir)

        # 5. Copy Inference Script to CODE directory
        logger.info("  Copying inference.py to code/...")
        shutil.copy(inference_script_src, os.path.join(code_dir, "inference.py"))

        # 6. Create requirements.txt in CODE directory ⭐ UPDATED REQUIREMENTS
        logger.info("  Creating requirements.txt in code/...")
        with open(os.path.join(code_dir, "requirements.txt"), "w") as req:
            req.write("xgboost\n")  # We need to install XGBoost
            req.write("joblib\n")   # Safe to include, or rely on base image
            req.write("scipy\n")    # Safe to include, or rely on base image

        # 7. Verify package structure before taring
        logger.info("  Package structure:")
        for root, dirs, files in os.walk(pkg_root):
            level = root.replace(pkg_root, '').count(os.sep)
            indent = ' ' * 2 * level
            logger.info(f"  {indent}{os.path.basename(root)}/")
            subindent = ' ' * 2 * (level + 1)
            for file in files:
                logger.info(f"  {subindent}{file}")

        # 8. Create Final Tar with correct structure
        out_tar = os.path.join(args.output_dir, f"{tgt}.tar.gz")
        
        logger.info("  Creating tar: %s", out_tar)
        with tarfile.open(out_tar, "w:gz") as tar:
            # Add booster at root (arcname ensures it's at root of tar)
            tar.add(
                os.path.join(pkg_root, "xgboost-model"), 
                arcname="xgboost-model"
            )
            # Add entire code directory (arcname ensures it's at root/code/)
            tar.add(code_dir, arcname="code")

        logger.info("✅ Packaged %s successfully at %s", tgt, out_tar)
        
        # Verify the tar contents
        logger.info("  Verifying tar contents:")
        with tarfile.open(out_tar, "r:gz") as tar:
            for member in tar.getmembers():
                logger.info(f"    {member.name}")
        
        # Clean up temp directory
        shutil.rmtree(pkg_root)

    logger.info("="*60)
    logger.info("✅ All targets processed successfully!")
    logger.info("="*60)

if __name__ == "__main__":
    main()