%%writefile repack_for_mme.py
import argparse
import os
import tarfile
import shutil
import glob

def pack(source_dir, output_tar):
    with tarfile.open(output_tar, "w:gz") as tar:
        tar.add(source_dir, arcname=".")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--models_dir")
    parser.add_argument("--preprocess_dir")
    parser.add_argument("--output_dir")
    args = parser.parse_args()

    os.makedirs(args.output_dir, exist_ok=True)

    # collect preprocess artifacts dynamically
    preprocess_artifacts = glob.glob(os.path.join(args.preprocess_dir, "*.pkl"))
    meta_file = os.path.join(args.preprocess_dir, "preprocess_meta.json")

    for tgt in os.listdir(args.models_dir):
        tgt_model_dir = os.path.join(args.models_dir, tgt)
        if not os.path.isdir(tgt_model_dir):
            continue

        packaging_dir = os.path.join(args.output_dir, tgt)
        os.makedirs(packaging_dir, exist_ok=True)

        # copy model files
        shutil.copy(os.path.join(tgt_model_dir, "xgboost-model"), packaging_dir)
        shutil.copy(os.path.join(tgt_model_dir, "class_map.json"), packaging_dir)

        # copy preprocess artifacts (dynamic)
        for art in preprocess_artifacts:
            shutil.copy(art, packaging_dir)

        # copy metadata
        if os.path.exists(meta_file):
            shutil.copy(meta_file, packaging_dir)

        # copy inference script
        shutil.copy("inference.py", packaging_dir)

        # create tar.gz
        pack(
            packaging_dir,
            os.path.join(args.output_dir, f"{tgt}.tar.gz")
        )

if __name__ == "__main__":
    main()
