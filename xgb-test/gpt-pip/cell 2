%%writefile preprocess.py
import argparse
import os
import glob
import pandas as pd
import numpy as np
import joblib
import json
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.preprocessing import LabelEncoder
from scipy import sparse


# -----------------------------
# Parse arguments
# -----------------------------
parser = argparse.ArgumentParser()
parser.add_argument("--input_dir")
parser.add_argument("--targets")
parser.add_argument("--input_features")
parser.add_argument("--output_dir")
parser.add_argument("--tfidf_max_features")
args = parser.parse_args()

targets = json.loads(args.targets)
input_features = json.loads(args.input_features)
output_dir = args.output_dir
hash_dim = int(args.tfidf_max_features)

os.makedirs(output_dir, exist_ok=True)


# -----------------------------
# Auto-detect CSV
# -----------------------------
csv_files = glob.glob(os.path.join(args.input_dir, "*.csv"))
if not csv_files:
    raise FileNotFoundError(f"No CSV found in {args.input_dir}")

input_csv = csv_files[0]
print(f"ðŸ“„ Using input CSV: {input_csv}")

df = pd.read_csv(input_csv)


# -----------------------------
# Validate required columns
# -----------------------------
missing = [c for c in input_features + targets if c not in df.columns]
if missing:
    raise ValueError(f"Dataset missing required columns: {missing}")


# -----------------------------
# Build dynamic feature pipeline
# -----------------------------
feature_blocks = []
feature_pipeline = []


for feat in input_features:
    col = df[feat]

    # TEXT FEATURES
    if col.dtype == object and (
        "desc" in feat.lower() or "text" in feat.lower() or "line" in feat.lower()
    ):
        print(f"ðŸ”¹ HashingVectorizer text feature: {feat}")

        vec = HashingVectorizer(
            n_features=hash_dim,
            alternate_sign=False,
            norm=None
        )

        X = vec.transform(col.fillna("").astype(str))
        feature_blocks.append(X)

        feature_pipeline.append({
            "name": feat,
            "type": "hash",
            "hash_dim": hash_dim
        })
        continue

    # CATEGORICAL FEATURES
    if col.dtype == object:
        print(f"ðŸ”¹ Label encoding categorical: {feat}")

        enc = LabelEncoder()
        df[feat] = df[feat].fillna("").astype(str)
        y = enc.fit_transform(df[feat])

        # encode as sparse column
        X = sparse.csr_matrix(y.reshape(-1, 1))
        feature_blocks.append(X)

        enc_file = f"labelenc_{feat}.pkl"
        joblib.dump(enc, os.path.join(output_dir, enc_file))

        feature_pipeline.append({
            "name": feat,
            "type": "labelenc",
            "file": enc_file
        })
        continue

    # NUMERIC FEATURES
    print(f"ðŸ”¹ Numeric feature: {feat}")
    y = pd.to_numeric(col.fillna(0), errors="coerce").fillna(0).values.reshape(-1, 1)
    X = sparse.csr_matrix(y)
    feature_blocks.append(X)

    feature_pipeline.append({
        "name": feat,
        "type": "numeric"
    })


# -----------------------------
# Merge sparse feature blocks â†’ sparse matrix
# -----------------------------
X_full = sparse.hstack(feature_blocks).tocsr()


# -----------------------------
# Save metadata
# -----------------------------
meta = {
    "feature_pipeline": feature_pipeline,
    "targets": targets
}

with open(os.path.join(output_dir, "preprocess_meta.json"), "w") as f:
    json.dump(meta, f, indent=2)


# -----------------------------
# Create per-target splits
# -----------------------------
X_full_np = X_full.toarray()   # final materialization only ONCE

for tgt in targets:
    print(f"ðŸ”§ Processing target: {tgt}")

    df_t = df[df[tgt].notna()].copy()
    y_t = df_t[tgt].astype(str).values

    mask = df[tgt].notna().values
    X_t = X_full_np[mask]

    n = len(y_t)
    cut = int(n * 0.8)

    X_train = X_t[:cut]
    y_train = y_t[:cut]

    X_val = X_t[cut:]
    y_val = y_t[cut:]

    tgt_dir = os.path.join(output_dir, tgt)
    os.makedirs(tgt_dir, exist_ok=True)

    train_df = pd.DataFrame(X_train)
    train_df["label"] = y_train
    train_df.to_csv(os.path.join(tgt_dir, "train.csv"), index=False)

    val_df = pd.DataFrame(X_val)
    val_df["label"] = y_val
    val_df.to_csv(os.path.join(tgt_dir, "val.csv"), index=False)

print("âœ… Preprocess complete. HashingVectorizer + metadata saved.")
