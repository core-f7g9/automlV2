%%writefile preprocess.py
import argparse
import os
import glob
import pandas as pd
import numpy as np
import joblib
import json
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.preprocessing import LabelEncoder
from scipy import sparse
from sklearn.datasets import dump_svmlight_file

# -----------------------------
# Helpers
# -----------------------------

def valid_target_mask(series):
    """
    Return a boolean mask where the target value is considered *valid*.

    Treated as MISSING (invalid for supervised training):
      - NaN / None
      - empty string ""
      - "0", "0.0"
      - textual placeholders: "nan", "NaN", "none", "None"
      - "null", "NULL"
      - whitespace-only values
    """

    s = series
    s_str = s.astype(str).str.strip()

    missing_values = {
        "", "0", "0.0",
        "nan", "NaN",
        "none", "None",
        "null", "NULL"
    }

    missing = s.isna() | s_str.isin(missing_values)

    return ~missing  # True where target is real, not missing


# -----------------------------
# Argparse
# -----------------------------

parser = argparse.ArgumentParser()
parser.add_argument("--input_dir")
parser.add_argument("--targets")
parser.add_argument("--input_features")
parser.add_argument("--output_dir")
parser.add_argument("--tfidf_max_features")
args = parser.parse_args()

targets = json.loads(args.targets)
input_features = json.loads(args.input_features)
output_dir = args.output_dir
hash_dim = int(args.tfidf_max_features)

os.makedirs(output_dir, exist_ok=True)

# -----------------------------
# Load dataset
# -----------------------------
csv_files = glob.glob(os.path.join(args.input_dir, "*.csv"))
if not csv_files:
    raise FileNotFoundError("No CSV found")

df = pd.read_csv(csv_files[0])

print(f"[preprocess] Loaded {csv_files[0]} with {len(df)} rows")

# -----------------------------
# Validate columns
# -----------------------------
missing = [c for c in input_features + targets if c not in df.columns]
if missing:
    raise ValueError(f"Missing required columns: {missing}")

# -----------------------------
# Build sparse feature blocks
# -----------------------------
feature_blocks = []
feature_pipeline = []

for feat in input_features:
    col = df[feat]

    # TEXT → HASHING
    if col.dtype == object and (
        "text" in feat.lower() or
        "desc" in feat.lower() or
        "line" in feat.lower()
    ):
        print(f"[features] {feat}: treating as TEXT → HashingVectorizer({hash_dim})")
        vec = HashingVectorizer(
            n_features=hash_dim,
            alternate_sign=False,
            norm=None
        )
        X = vec.transform(col.fillna("").astype(str))
        feature_blocks.append(X)

        feature_pipeline.append({
            "name": feat,
            "type": "hash",
            "hash_dim": hash_dim
        })
        continue

    # CATEGORICAL → LABELENC
    if col.dtype == object:
        print(f"[features] {feat}: treating as CATEGORICAL → LabelEncoder")
        enc = LabelEncoder()
        df[feat] = df[feat].fillna("").astype(str)
        encoded = enc.fit_transform(df[feat])
        X = sparse.csr_matrix(encoded.reshape(-1, 1))
        feature_blocks.append(X)

        enc_file = f"labelenc_{feat}.pkl"
        joblib.dump(enc, os.path.join(output_dir, enc_file))

        feature_pipeline.append({
            "name": feat,
            "type": "labelenc",
            "file": enc_file
        })
        continue

    # NUMERIC
    print(f"[features] {feat}: treating as NUMERIC")
    y = pd.to_numeric(col.fillna(0), errors="coerce").fillna(0).values.reshape(-1, 1)
    X = sparse.csr_matrix(y)
    feature_blocks.append(X)

    feature_pipeline.append({
        "name": feat,
        "type": "numeric"
    })

# -----------------------------
# Merge sparse blocks
# -----------------------------
print("[features] Merging feature blocks into sparse matrix")
X_full = sparse.hstack(feature_blocks).tocsr()

# -----------------------------
# Save metadata
# -----------------------------
meta = {"feature_pipeline": feature_pipeline, "targets": targets}
meta_path = os.path.join(output_dir, "preprocess_meta.json")
with open(meta_path, "w") as f:
    json.dump(meta, f, indent=2)
print(f"[meta] Wrote {meta_path}")

# -----------------------------
# Per-target splitting (STILL SPARSE)
# -----------------------------
for tgt in targets:
    print("-" * 60)
    print(f"[target] Processing target: {tgt}")

    col = df[tgt]
    mask_valid = valid_target_mask(col)

    n_total = len(col)
    n_valid = int(mask_valid.sum())
    n_missing = n_total - n_valid

    print(f"[target] Total rows:   {n_total}")
    print(f"[target] Valid rows:   {n_valid}")
    print(f"[target] Missing/0s:   {n_missing}")

    if n_valid < 10:
        print(f"[target] WARNING: only {n_valid} valid rows for {tgt}. "
              f"Model may be very weak.")

    # Filter to rows with a *real* target (as string)
    y_all_raw = df.loc[mask_valid, tgt].astype(str).values
    X_t = X_full[mask_valid]

    n = len(y_all_raw)
    if n == 0:
        print(f"[target] No valid rows after filtering for {tgt}, skipping.")
        continue

    # ------------------------------------------------------------------
    # NEW: Ensure labels are numeric for LIBSVM
    # ------------------------------------------------------------------
    tgt_dir = os.path.join(output_dir, tgt)
    os.makedirs(tgt_dir, exist_ok=True)

    # First try: can we interpret labels as numeric directly?
    try:
        y_all_num = pd.to_numeric(y_all_raw, errors="raise").astype(float)
        label_map = None  # Not needed, labels already numeric
        print(f"[target] {tgt}: labels are numeric-compatible")
    except Exception:
        # Fallback: treat labels as categorical and map to 0..K-1
        print(f"[target] {tgt}: labels are non-numeric, encoding to integers")
        unique_labels = sorted(pd.unique(y_all_raw))
        label_map = {str(lbl): int(i) for i, lbl in enumerate(unique_labels)}
        y_all_num = np.array([label_map[str(lbl)] for lbl in y_all_raw], dtype=float)

        # Optional but useful: save mapping for this target
        map_path = os.path.join(tgt_dir, "label_map.json")
        with open(map_path, "w") as f:
            json.dump(label_map, f, indent=2)
        print(f"[target] Saved label_map.json with {len(label_map)} entries to {map_path}")

    # ------------------------------------------------------------------
    # 80/20 split using numeric labels
    # ------------------------------------------------------------------
    cut = int(n * 0.8)
    if cut == 0:
        cut = max(1, n - 1)  # ensure at least 1 train, 1 val if possible

    X_train = X_t[:cut]
    y_train = y_all_num[:cut]

    X_val = X_t[cut:]
    y_val = y_all_num[cut:]

    print(f"[target] Train rows: {len(y_train)}, Val rows: {len(y_val)}")

    train_path = os.path.join(tgt_dir, "train.libsvm")
    val_path = os.path.join(tgt_dir, "val.libsvm")

    dump_svmlight_file(X_train, y_train, train_path)
    dump_svmlight_file(X_val,   y_val,   val_path)

    print(f"[target] Wrote {train_path}")
    print(f"[target] Wrote {val_path}")

print("✅ Preprocess complete — targets now safely numeric for LIBSVM.")

