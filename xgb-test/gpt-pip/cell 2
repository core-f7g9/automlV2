%%writefile preprocess.py
import argparse
import os
import glob
import pandas as pd
import numpy as np
import joblib
import json
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.preprocessing import LabelEncoder
from scipy import sparse
from sklearn.datasets import dump_svmlight_file


parser = argparse.ArgumentParser()
parser.add_argument("--input_dir")
parser.add_argument("--targets")
parser.add_argument("--input_features")
parser.add_argument("--output_dir")
parser.add_argument("--tfidf_max_features")
args = parser.parse_args()

targets = json.loads(args.targets)
input_features = json.loads(args.input_features)
output_dir = args.output_dir
hash_dim = int(args.tfidf_max_features)

os.makedirs(output_dir, exist_ok=True)

# -----------------------------
# Load dataset
# -----------------------------
csv_files = glob.glob(os.path.join(args.input_dir, "*.csv"))
if not csv_files:
    raise FileNotFoundError("No CSV found")

df = pd.read_csv(csv_files[0])

# -----------------------------
# Validate columns
# -----------------------------
missing = [c for c in input_features + targets if c not in df.columns]
if missing:
    raise ValueError(f"Missing required columns: {missing}")


# -----------------------------
# Build sparse feature blocks
# -----------------------------
feature_blocks = []
feature_pipeline = []
encoders = {}

for feat in input_features:
    col = df[feat]

    # TEXT → HASHING
    if col.dtype == object and (
        "text" in feat.lower() or
        "desc" in feat.lower() or
        "line" in feat.lower()
    ):
        vec = HashingVectorizer(
            n_features=hash_dim,
            alternate_sign=False,
            norm=None
        )
        X = vec.transform(col.fillna("").astype(str))
        feature_blocks.append(X)

        feature_pipeline.append({
            "name": feat,
            "type": "hash",
            "hash_dim": hash_dim
        })
        continue

    # CATEGORICAL → LABELENC
    if col.dtype == object:
        enc = LabelEncoder()
        df[feat] = df[feat].fillna("").astype(str)
        encoded = enc.fit_transform(df[feat])
        encoders[feat] = enc

        X = sparse.csr_matrix(encoded.reshape(-1, 1))
        feature_blocks.append(X)

        enc_file = f"labelenc_{feat}.pkl"
        joblib.dump(enc, os.path.join(output_dir, enc_file))

        feature_pipeline.append({
            "name": feat,
            "type": "labelenc",
            "file": enc_file
        })
        continue

    # NUMERIC
    y = pd.to_numeric(col.fillna(0), errors="coerce").fillna(0).values.reshape(-1, 1)
    X = sparse.csr_matrix(y)
    feature_blocks.append(X)

    feature_pipeline.append({
        "name": feat,
        "type": "numeric"
    })


# -----------------------------
# Merge sparse blocks
# -----------------------------
X_full = sparse.hstack(feature_blocks).tocsr()   # <-- stays sparse ALWAYS


# -----------------------------
# Save metadata
# -----------------------------
meta = {"feature_pipeline": feature_pipeline, "targets": targets}
with open(os.path.join(output_dir, "preprocess_meta.json"), "w") as f:
    json.dump(meta, f, indent=2)


# -----------------------------
# Per-target splitting (STILL SPARSE)
# -----------------------------
for tgt in targets:

    mask = df[tgt].notna().values
    y_all = df.loc[mask, tgt].astype(str).values

    X_t = X_full[mask]   # sparse slice, SAFE

    n = len(y_all)
    cut = int(n * 0.8)

    X_train = X_t[:cut]
    y_train = y_all[:cut]

    X_val = X_t[cut:]
    y_val = y_all[cut:]

    tgt_dir = os.path.join(output_dir, tgt)
    os.makedirs(tgt_dir, exist_ok=True)

    dump_svmlight_file(X_train, y_train, os.path.join(tgt_dir, "train.libsvm"))
    dump_svmlight_file(X_val,   y_val,   os.path.join(tgt_dir, "val.libsvm"))

print("✅ Preprocess complete — fully sparse, no dense memory usage.")
