%%writefile preprocess.py
import argparse
import os
import glob
import pandas as pd
import numpy as np
import joblib
import json
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.preprocessing import LabelEncoder
from scipy import sparse
from sklearn.datasets import dump_svmlight_file
from sklearn.model_selection import train_test_split

def valid_target_mask(series):
    s = series
    s_str = s.astype(str).str.strip()
    missing_values = {"", "nan", "NaN", "none", "None", "null", "NULL"}
    missing = s.isna() | s_str.isin(missing_values)
    return ~missing

def can_stratify(y, min_per_class=2):
    # Need at least 2 samples per class for stratified splitting
    _, counts = np.unique(y, return_counts=True)
    return (counts.min() >= min_per_class)

parser = argparse.ArgumentParser()
parser.add_argument("--input_dir")
parser.add_argument("--targets")
parser.add_argument("--input_features")
parser.add_argument("--output_dir")
parser.add_argument("--hash_dim", type=int, default=2000)
parser.add_argument("--test_size", type=float, default=0.2)
parser.add_argument("--random_state", type=int, default=42)
args = parser.parse_args()

targets = json.loads(args.targets)
input_features = json.loads(args.input_features)
output_dir = args.output_dir
hash_dim = int(args.hash_dim)

os.makedirs(output_dir, exist_ok=True)

csv_files = glob.glob(os.path.join(args.input_dir, "*.csv"))
if not csv_files:
    raise FileNotFoundError("No CSV found in input_dir")

df = pd.read_csv(csv_files[0])
print(f"[preprocess] Loaded {csv_files[0]} with {len(df)} rows")

missing_cols = [c for c in input_features + targets if c not in df.columns]
if missing_cols:
    raise ValueError(f"Missing required columns: {missing_cols}")

feature_blocks = []
feature_pipeline = []

for feat in input_features:
    col = df[feat]

    # TEXT -> Hashing
    if col.dtype == object and (
        "text" in feat.lower() or "desc" in feat.lower() or "line" in feat.lower()
    ):
        print(f"[features] {feat}: TEXT -> HashingVectorizer({hash_dim})")
        vec = HashingVectorizer(
            n_features=hash_dim,
            alternate_sign=False,
            norm=None
        )
        X = vec.transform(col.fillna("").astype(str))
        feature_blocks.append(X)
        feature_pipeline.append({"name": feat, "type": "hash", "hash_dim": hash_dim})
        continue

    # CATEGORICAL -> LabelEncoder (kept to match your current server)
    if col.dtype == object:
        print(f"[features] {feat}: CATEGORICAL -> LabelEncoder")
        enc = LabelEncoder()
        df[feat] = df[feat].fillna("").astype(str)
        encoded = enc.fit_transform(df[feat])
        X = sparse.csr_matrix(encoded.reshape(-1, 1))
        feature_blocks.append(X)

        enc_file = f"labelenc_{feat}.pkl"
        joblib.dump(enc, os.path.join(output_dir, enc_file))
        feature_pipeline.append({"name": feat, "type": "labelenc", "file": enc_file})
        continue

    # NUMERIC
    print(f"[features] {feat}: NUMERIC")
    y = pd.to_numeric(col.fillna(0), errors="coerce").fillna(0).values.reshape(-1, 1)
    X = sparse.csr_matrix(y)
    feature_blocks.append(X)
    feature_pipeline.append({"name": feat, "type": "numeric"})

print("[features] Merging sparse blocks")
X_full = sparse.hstack(feature_blocks).tocsr()

meta = {"feature_pipeline": feature_pipeline, "targets": targets}
meta_path = os.path.join(output_dir, "preprocess_meta.json")
with open(meta_path, "w") as f:
    json.dump(meta, f, indent=2)
print(f"[meta] Wrote {meta_path}")

# Per-target outputs
for tgt in targets:
    print("-" * 60)
    print(f"[target] {tgt}")

    col = df[tgt]
    mask_valid = valid_target_mask(col)

    y_raw = df.loc[mask_valid, tgt].astype(str).values
    X_t = X_full[mask_valid]

    n = len(y_raw)
    print(f"[target] Valid rows: {n} / {len(df)}")

    if n == 0:
        print(f"[target] No valid rows for {tgt}, skipping")
        continue

    # Unified label mapping
    unique_labels = sorted(pd.unique(y_raw))
    label_map = {str(lbl): int(i) for i, lbl in enumerate(unique_labels)}
    y_num = np.array([label_map[str(lbl)] for lbl in y_raw], dtype=int)

    tgt_dir = os.path.join(output_dir, tgt)
    os.makedirs(tgt_dir, exist_ok=True)

    with open(os.path.join(tgt_dir, "label_map.json"), "w") as f:
        json.dump(label_map, f, indent=2)

    stratify = y_num if can_stratify(y_num) else None
    if stratify is None:
        print("[split] Not stratifying (rare classes). Using random split.")
    else:
        print("[split] Stratified split enabled.")

    X_train, X_val, y_train, y_val = train_test_split(
        X_t, y_num,
        test_size=args.test_size,
        random_state=args.random_state,
        shuffle=True,
        stratify=stratify
    )

    train_path = os.path.join(tgt_dir, "train.libsvm")
    val_path   = os.path.join(tgt_dir, "val.libsvm")

    dump_svmlight_file(X_train, y_train, train_path)
    dump_svmlight_file(X_val,   y_val,   val_path)

    print(f"[target] Train={len(y_train)} Val={len(y_val)}")
    print(f"[target] Wrote {train_path}")
    print(f"[target] Wrote {val_path}")

print("âœ… Preprocess complete")
