# ============================
# Endpoint evaluation
# (STRICTLY on values the model was trained on)
# ============================
import boto3
import json
import pandas as pd
import sagemaker

# ----------------------------
# Reuse globals if present
# ----------------------------
region = region if "region" in globals() else boto3.Session().region_name
sm_sess = sm_session if "sm_session" in globals() else sagemaker.Session()
BUCKET = BUCKET if "BUCKET" in globals() else sm_sess.default_bucket()
PROJECT_PREFIX = PROJECT_PREFIX if "PROJECT_PREFIX" in globals() else "multi-target-xgb"

INPUT_S3 = INPUT_S3 if "INPUT_S3" in globals() else f"s3://{BUCKET}/{PROJECT_PREFIX}/input/data.csv"

TARGET_COLS = TARGET_COLS if "TARGET_COLS" in globals() else [
    "DepartmentCode",
    "AccountCode",
    "SubAccountCode",
    "LocationCode",
]

INPUT_FEATURES = INPUT_FEATURES if "INPUT_FEATURES" in globals() else [
    "VendorName",
    "LineDescription",
    "ClubNumber",
]

endpoint_name = f"{PROJECT_PREFIX}-custom-endpoint"
runtime = boto3.client("sagemaker-runtime", region_name=region)

# ----------------------------
# Helpers (MATCH TRAINING LOGIC)
# ----------------------------
def valid_target_mask(series: pd.Series) -> pd.Series:
    """
    EXACTLY mirrors the training-side exclusion logic.
    These values were NOT trained on and MUST be excluded.
    """
    s = series
    s_str = s.astype(str).str.strip()

    missing_values = {
        "", "0", "0.0",
        "nan", "NaN",
        "none", "None",
        "null", "NULL",
    }

    missing = s.isna() | s_str.isin(missing_values)
    return ~missing

def normalize_str(x: pd.Series) -> pd.Series:
    return x.astype(str).str.strip()

# ----------------------------
# Load dataset
# ----------------------------
print("Reading data from:", INPUT_S3)
df = pd.read_csv(INPUT_S3)
print("Total rows in dataset:", len(df))

# Validate schema
required_cols = set(TARGET_COLS + INPUT_FEATURES)
missing_cols = [c for c in required_cols if c not in df.columns]
if missing_cols:
    raise ValueError(f"Missing required columns: {missing_cols}")

# ----------------------------
# Evaluation
# ----------------------------
N_SAMPLES = 500     # increase if dataset allows
RANDOM_STATE = 42

results = {}

for tgt in TARGET_COLS:
    print("\n" + "=" * 70)
    print(f"Target: {tgt}")

    mask = valid_target_mask(df[tgt])
    df_t = df.loc[mask].copy()

    if df_t.empty:
        print("  No rows after filtering; skipping.")
        continue

    n = min(N_SAMPLES, len(df_t))
    sample = df_t.sample(n=n, random_state=RANDOM_STATE).copy()

    instances = sample[INPUT_FEATURES].to_dict(orient="records")
    payload = {"target": tgt, "instances": instances}

    response = runtime.invoke_endpoint(
        EndpointName=endpoint_name,
        ContentType="application/json",
        Body=json.dumps(payload),
    )

    resp_json = json.loads(response["Body"].read().decode("utf-8"))

    # Support both wrapped and flat responses
    if isinstance(resp_json, dict) and "predictions" in resp_json:
        preds = [p["prediction"] for p in resp_json["predictions"]]
    elif isinstance(resp_json, list):
        preds = [p["prediction"] for p in resp_json]
    else:
        raise ValueError(f"Unexpected response format for {tgt}")

    if len(preds) != len(sample):
        raise ValueError("Prediction count mismatch")

    sample["pred"] = preds

    gt = normalize_str(sample[tgt])
    pd_ = normalize_str(sample["pred"])

    accuracy = (gt == pd_).mean()

    # Majority-class baseline (on SAME filtered rows)
    majority_label = gt.value_counts().idxmax()
    baseline = (gt == majority_label).mean()

    results[tgt] = {
        "accuracy": float(accuracy),
        "baseline": float(baseline),
        "rows": int(n),
        "classes": int(gt.nunique()),
    }

    print(f"  Rows evaluated:        {n}")
    print(f"  Classes evaluated:     {gt.nunique()}")
    print(f"  Accuracy:              {accuracy:.4f}")
    print(f"  Majority baseline:     {baseline:.4f}")
    print("  Sample predictions:")
    print(sample[[*INPUT_FEATURES, tgt, "pred"]].head(10))

# ----------------------------
# Summary
# ----------------------------
print("\n" + "=" * 70)
print("SUMMARY (NON-ZERO / NON-NULL ONLY)")
for tgt, m in results.items():
    print(
        f"{tgt}: "
        f"accuracy={m['accuracy']:.4f}, "
        f"baseline={m['baseline']:.4f}, "
        f"rows={m['rows']}, "
        f"classes={m['classes']}"
    )
