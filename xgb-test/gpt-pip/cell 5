%%writefile inference.py
import json
import os
import logging
import pickle
import numpy as np
import xgboost as xgb
import sys

# ---------------------------------------------------------
# DEFENSIVE CODING:
# Ensure 'code' directory is in path just in case
# ---------------------------------------------------------
current_path = os.path.dirname(os.path.abspath(__file__))
if current_path not in sys.path:
    sys.path.append(current_path)

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

def model_fn(model_dir):
    """
    Load the model and artifacts for MME.
    """
    logger.info("="*50)
    logger.info(f"Loading model from: {model_dir}")
    
    # 1. Load XGBoost Booster
    # In MME, model_dir is usually /opt/ml/models/<target_model_name>/
    model_path = os.path.join(model_dir, "xgboost-model")
    
    # Fallback search if not at root
    if not os.path.exists(model_path):
        logger.info("xgboost-model not at root, searching...")
        for root, dirs, files in os.walk(model_dir):
            if "xgboost-model" in files:
                model_path = os.path.join(root, "xgboost-model")
                logger.info(f"Found at: {model_path}")
                break
    
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"xgboost-model not found in {model_dir}")

    booster = xgb.Booster()
    booster.load_model(model_path)
    logger.info("âœ… Booster loaded successfully")
    
    # 2. Load artifacts from code directory
    # If this script is running, we are likely INSIDE the code directory or it is adjacent
    code_dir = os.path.join(model_dir, "code")
    
    # Fallback: if we can't find code dir via model_dir, assume we are inside it
    if not os.path.exists(code_dir):
        logger.info("code directory not found at root, assuming current directory contains artifacts")
        code_dir = current_path

    # Load class map
    class_map_path = os.path.join(code_dir, "class_map.json")
    with open(class_map_path, "r") as f:
        class_map = json.load(f)

    # Load metadata
    meta_path = os.path.join(code_dir, "preprocess_meta.json")
    with open(meta_path, "r") as f:
        meta = json.load(f)
    
    pipeline = meta["feature_pipeline"]

    # Load Label Encoders
    encoders = {}
    for step in pipeline:
        if step["type"] == "labelenc":
            enc_path = os.path.join(code_dir, step["file"])
            if os.path.exists(enc_path):
                with open(enc_path, "rb") as f:
                    encoders[step["name"]] = pickle.load(f)

    return {
        "model": booster,
        "class_map": class_map,
        "pipeline": pipeline,
        "encoders": encoders
    }

def input_fn(request_body, request_content_type):
    if request_content_type == "application/json":
        return json.loads(request_body)
    raise ValueError("Content type must be application/json")

def predict_fn(data, artifacts):
    # DELAYED IMPORT: Import sklearn here to ensure requirements.txt 
    # has definitely been processed by the container
    from sklearn.feature_extraction.text import HashingVectorizer
    
    booster = artifacts["model"]
    pipeline = artifacts["pipeline"]
    encoders = artifacts["encoders"]
    class_map = artifacts["class_map"]

    if isinstance(data, dict):
        data = [data]
        single_request = True
    else:
        single_request = False

    batch_features = []

    for row in data:
        features = []
        for step in pipeline:
            name = step["name"]
            ftype = step["type"]
            raw = row.get(name, "")

            if ftype == "hash":
                vec = HashingVectorizer(n_features=step["hash_dim"], alternate_sign=False, norm=None)
                # HashingVectorizer expects iterable, so wrap raw in list
                arr = vec.transform([str(raw)]).toarray()[0]
                features.append(arr)
            elif ftype == "labelenc":
                enc = encoders.get(name)
                val = 0
                if enc:
                    try:
                        val = enc.transform([str(raw)])[0]
                    except:
                        val = 0
                features.append(np.array([val]))
            else:
                try:
                    val = float(raw)
                except:
                    val = 0.0
                features.append(np.array([val]))

        full_row = np.hstack(features)
        batch_features.append(full_row)

    X = np.array(batch_features)
    dtest = xgb.DMatrix(X)
    probs_batch = booster.predict(dtest)
    
    inv_map = {v: k for k, v in class_map.items()}
    results = []
    
    # Handle both binary (1D array) and multi-class (2D array) outputs
    if len(probs_batch.shape) == 1:
        # If binary, xgboost often outputs probability of class 1
        # You might need logic here depending on your objective
        pass 
        
    for probs in probs_batch:
        idx = int(np.argmax(probs))
        pred_label = inv_map.get(idx, str(idx))
        results.append({
            "prediction": pred_label,
            "probabilities": probs.tolist()
        })

    if single_request:
        return results[0]
    return results

def output_fn(prediction, accept):
    return json.dumps(prediction)