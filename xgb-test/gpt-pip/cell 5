%%writefile inference.py
import json
import numpy as np
import xgboost as xgb
import os
import pickle
from sklearn.feature_extraction.text import TfidfVectorizer

def model_fn(model_dir):

    # load boosted model
    booster = xgb.Booster()
    booster.load_model(os.path.join(model_dir, "xgboost-model"))

    # load class map
    with open(os.path.join(model_dir, "class_map.json")) as f:
        class_map = json.load(f)

    # load preprocess metadata
    meta_path = os.path.join(model_dir, "preprocess_meta.json")
    with open(meta_path) as f:
        meta = json.load(f)

    feature_pipeline = meta["feature_pipeline"]

    # load artifacts dynamically
    artifacts = {}
    for step in feature_pipeline:
        if step["type"] == "tfidf":
            with open(os.path.join(model_dir, step["file"]), "rb") as f:
                vocab = pickle.load(f)
            artifacts[step["name"]] = {"type": "tfidf", "vocab": vocab}

        elif step["type"] == "labelenc":
            with open(os.path.join(model_dir, step["file"]), "rb") as f:
                enc = pickle.load(f)
            artifacts[step["name"]] = {"type": "labelenc", "encoder": enc}

        else:
            artifacts[step["name"]] = {"type": "numeric"}

    return {
        "model": booster,
        "class_map": class_map,
        "pipeline": feature_pipeline,
        "artifacts": artifacts
    }

def input_fn(request_body, request_content_type):
    return json.loads(request_body)

def predict_fn(data, model_artifacts):

    pipeline = model_artifacts["pipeline"]
    artifacts = model_artifacts["artifacts"]
    booster = model_artifacts["model"]

    feature_values = []

    for step in pipeline:
        name = step["name"]
        ftype = artifacts[name]["type"]

        raw_val = data[name]

        if ftype == "tfidf":
            vocab = artifacts[name]["vocab"]
            vec = TfidfVectorizer(vocabulary=vocab)
            X = vec.fit_transform([str(raw_val)]).toarray()[0]
            feature_values.append(X)

        elif ftype == "labelenc":
            enc = artifacts[name]["encoder"]
            val = enc.transform([str(raw_val)])[0]
            feature_values.append(np.array([val]))

        else:  # numeric
            val = float(raw_val)
            feature_values.append(np.array([val]))

    X_full = np.hstack(feature_values)
    dmat = xgb.DMatrix(X_full.reshape(1, -1))

    probs = booster.predict(dmat)[0]
    idx_to_class = {v: k for k, v in model_artifacts["class_map"].items()}
    pred = idx_to_class[int(np.argmax(probs))]

    return {"prediction": pred, "probabilities": probs.tolist()}

def output_fn(prediction, accept):
    return json.dumps(prediction)
