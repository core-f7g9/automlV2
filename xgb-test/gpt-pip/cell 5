%%writefile inference.py
import json
import os
import logging

import numpy as np
import pickle
import xgboost as xgb
from sklearn.feature_extraction.text import HashingVectorizer

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

def model_fn(model_dir):
    logger.info("=== model_fn called model_dir=%s ===", model_dir)

    model_path = os.path.join(model_dir, "model", "xgboost-model")
    code_dir = os.path.join(model_dir, "code")

    # Load booster
    booster = xgb.Booster()
    booster.load_model(model_path)
    logger.info("Loaded booster OK")

    # Load class map
    with open(os.path.join(code_dir, "class_map.json")) as f:
        class_map = json.load(f)

    # Load metadata
    with open(os.path.join(code_dir, "preprocess_meta.json")) as f:
        meta = json.load(f)

    pipeline = meta["feature_pipeline"]

    # Load all encoders
    encoders = {}
    for step in pipeline:
        if step["type"] == "labelenc":
            enc_path = os.path.join(code_dir, step["file"])
            with open(enc_path, "rb") as f:
                encoders[step["name"]] = pickle.load(f)

    return {
        "model": booster,
        "class_map": class_map,
        "pipeline": pipeline,
        "encoders": encoders
    }

def input_fn(request_body, request_content_type):
    return json.loads(request_body)

def predict_fn(data, artifacts):
    booster = artifacts["model"]
    pipeline = artifacts["pipeline"]
    encoders = artifacts["encoders"]
    class_map = artifacts["class_map"]

    features = []

    for step in pipeline:
        name = step["name"]
        ftype = step["type"]
        raw = data.get(name, "")

        if ftype == "hash":
            vec = HashingVectorizer(
                n_features=step["hash_dim"],
                alternate_sign=False,
                norm=None
            )
            arr = vec.transform([str(raw)]).toarray()[0]
            features.append(arr)
        elif ftype == "labelenc":
            enc = encoders[name]
            val = enc.transform([str(raw)])[0]
            features.append(np.array([val]))
        else:
            val = float(raw) if raw not in ("", None) else 0.0
            features.append(np.array([val]))

    X = np.hstack(features).reshape(1, -1)
    probs = booster.predict(xgb.DMatrix(X))[0]
    idx = int(np.argmax(probs))

    inv = {v: k for k, v in class_map.items()}

    return {"prediction": inv[idx], "probabilities": probs.tolist()}

def output_fn(prediction, accept):
    return json.dumps(prediction)
