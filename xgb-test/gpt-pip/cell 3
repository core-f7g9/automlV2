%%writefile train.py
import json
import xgboost as xgb
import os
import numpy as np

def main():
    # Fixed SageMaker channel paths for libsvm data
    train_path = "/opt/ml/input/data/train/train.libsvm"
    val_path   = "/opt/ml/input/data/val/val.libsvm"
    model_dir  = "/opt/ml/model"
    os.makedirs(model_dir, exist_ok=True)

    # -----------------------------
    # Load LIBSVM sparse data
    # -----------------------------
    dtrain = xgb.DMatrix(train_path)
    dval   = xgb.DMatrix(val_path)

    # -----------------------------
    # Determine num_class dynamically
    # -----------------------------
    train_labels = dtrain.get_label()
    unique_classes = sorted(set(train_labels))
    num_class = len(unique_classes)

    # Map original label values -> 0..K-1
    class_map = {str(cls): int(i) for i, cls in enumerate(unique_classes)}

    # Save class map for inference
    with open(os.path.join(model_dir, "class_map.json"), "w") as f:
        json.dump(class_map, f, indent=2)

    def encode_labels(dmat):
        y = dmat.get_label()
        mapped = np.array([class_map[str(lbl)] for lbl in y])
        dmat.set_label(mapped)

    encode_labels(dtrain)
    encode_labels(dval)

    # -----------------------------
    # Train XGBoost multi-class model
    # -----------------------------
    params = {
        "objective": "multi:softprob",
        "num_class": num_class,
        "eval_metric": "mlogloss",
        "tree_method": "hist",
        "max_depth": 8,
        "eta": 0.2,
    }

    watchlist = [(dtrain, "train"), (dval, "validation")]

    booster = xgb.train(
        params=params,
        dtrain=dtrain,
        num_boost_round=200,
        evals=watchlist,
    )

    # -----------------------------
    # Save model
    # -----------------------------
    booster.save_model(os.path.join(model_dir, "xgboost-model"))
    print("âœ… Training complete.")

if __name__ == "__main__":
    main()
