# ============================================================
# Cell 3: SKLearn train + inference script with hashing trick
# ============================================================
import os, textwrap

os.makedirs("sklearn_src", exist_ok=True)

model_script = textwrap.dedent("""
import os
import argparse
import io
import json
from typing import Optional, List, Tuple

import numpy as np
import pandas as pd
import joblib

from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction import FeatureHasher


# ------------------------
# Configurable hashing dims
# ------------------------
HASH_SIZE_TEXT = 1024    # high-cardinality free text
HASH_SIZE_CAT  = 256     # categorical short strings


# ------------------------
# Feature hashing utilities
# ------------------------

def hash_text_series(series: pd.Series, n_features: int) -> np.ndarray:
    \"\"\"Hash a text column into fixed-dim sparse matrix → dense numpy array.\"\"\"
    hasher = FeatureHasher(n_features=n_features, input_type="string")
    # convert values to strings, drop NaN
    data = series.fillna("__MISSING__").astype(str).tolist()
    sparse_matrix = hasher.transform([[x] for x in data])
    return sparse_matrix.toarray()


def process_features(
    df: pd.DataFrame,
    feature_columns: Optional[List[str]] = None,
    text_cols: Optional[List[str]] = None,
    cat_cols: Optional[List[str]] = None,
    num_cols: Optional[List[str]] = None,
    is_training: bool = True,
    training_meta: Optional[dict] = None
) -> Tuple[np.ndarray, dict]:
    \"\"\"
    Convert df → hashed feature matrix.
    Returns:
      X (np.ndarray), meta (dict containing column lists)
    \"\"\"

    # Identify column types on training
    if is_training:
        num_cols = df.select_dtypes(include=["number", "bool"]).columns.tolist()
        other_cols = [c for c in df.columns if c not in num_cols]

        # For simplicity: treat long text separately (LineDescription)
        text_cols = [c for c in other_cols if c.lower().endswith("description")]
        # Everything else becomes categorical hashed
        cat_cols = [c for c in other_cols if c not in text_cols]

        meta = {
            "num_cols": num_cols,
            "text_cols": text_cols,
            "cat_cols": cat_cols,
        }
    else:
        # use training-time metadata
        num_cols = training_meta["num_cols"]
        text_cols = training_meta["text_cols"]
        cat_cols = training_meta["cat_cols"]
        meta = training_meta

    # ---- Numeric features ----
    X_num = df[num_cols].fillna(0).to_numpy(dtype=np.float32)

    # ---- Categorical hashing ----
    X_cat_list = []
    for c in cat_cols:
        Xc = hash_text_series(df[c], HASH_SIZE_CAT)
        X_cat_list.append(Xc)

    # ---- Text hashing ----
    X_text_list = []
    for c in text_cols:
        Xt = hash_text_series(df[c], HASH_SIZE_TEXT)
        X_text_list.append(Xt)

    # ---- Concatenate all components ----
    parts = [X_num]
    if X_cat_list:
        parts.extend(X_cat_list)
    if X_text_list:
        parts.extend(X_text_list)

    X = np.concatenate(parts, axis=1).astype(np.float32)

    return X, meta


# ------------------------
# Training entrypoint
# ------------------------

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--target-name", type=str, required=True)
    args, _ = parser.parse_known_args()

    target_name = args.target_name

    train_dir = os.environ.get("SM_CHANNEL_TRAIN", "/opt/ml/input/data/train")
    val_dir   = os.environ.get("SM_CHANNEL_VALIDATION", "/opt/ml/input/data/validation")
    model_dir = os.environ.get("SM_MODEL_DIR", "/opt/ml/model")

    train_df = pd.read_csv(os.path.join(train_dir, "train.csv"), low_memory=False)
    val_df   = pd.read_csv(os.path.join(val_dir,  "validation.csv"), low_memory=False)

    # Split X, y
    y_train = train_df[target_name].astype(str)
    X_train_df = train_df.drop(columns=[target_name])

    y_val = val_df[target_name].astype(str)
    X_val_df = val_df.drop(columns=[target_name])

    # Featurize with hashing (training mode)
    X_train, fe_meta = process_features(
        df=X_train_df,
        is_training=True
    )

    # Featurize validation using same metadata
    X_val, _ = process_features(
        df=X_val_df,
        is_training=False,
        training_meta=fe_meta
    )

    print(f"[train] Feature matrix shapes: train={X_train.shape}, val={X_val.shape}")

    # ---- Train model ----
    clf = RandomForestClassifier(
        n_estimators=200,
        max_depth=None,
        n_jobs=-1,
        random_state=42
    )
    clf.fit(X_train, y_train)

    # ---- Validation metric ----
    if len(y_val) > 0:
        acc = (clf.predict(X_val) == y_val).mean()
        print(f"[train] Validation accuracy for {target_name}: {acc:.4f}")

    # ---- Save bundle ----
    bundle = {
        "model": clf,
        "fe_meta": fe_meta,
        "target_name": target_name,
    }

    os.makedirs(model_dir, exist_ok=True)
    out_path = os.path.join(model_dir, "model.joblib")
    joblib.dump(bundle, out_path)

    print(f"[train] Saved model bundle to {out_path}")


# ------------------------
# Inference handlers
# ------------------------

def model_fn(model_dir: str):
    bundle = joblib.load(os.path.join(model_dir, "model.joblib"))
    return bundle


def input_fn(input_data, content_type: str):
    if content_type == "text/csv":
        if isinstance(input_data, (bytes, bytearray)):
            input_data = input_data.decode("utf-8")
        return pd.read_csv(io.StringIO(input_data), header=0)

    if content_type == "application/json":
        obj = json.loads(input_data)
        if isinstance(obj, list):
            return pd.DataFrame(obj)
        return pd.DataFrame([obj])

    raise ValueError(f"Unsupported content_type: {content_type}")


def predict_fn(data: pd.DataFrame, model_bundle):
    target = model_bundle["target_name"]
    fe_meta = model_bundle["fe_meta"]
    clf = model_bundle["model"]

    # drop target column if present
    if target in data.columns:
        data = data.drop(columns=[target])

    X, _ = process_features(
        df=data,
        is_training=False,
        training_meta=fe_meta
    )

    preds = clf.predict(X)
    return preds


def output_fn(prediction, accept: str):
    if isinstance(prediction, (pd.Series, pd.DataFrame, np.ndarray)):
        preds = prediction.tolist()
    elif isinstance(prediction, list):
        preds = prediction
    else:
        preds = [prediction]

    if accept == "text/csv":
        return ",".join(str(x) for x in preds), "text/csv"

    return json.dumps(preds), "application/json"
""")

with open("sklearn_src/model_script.py", "w") as f:
    f.write(model_script)

print("Wrote sklearn_src/model_script.py")
