# ============================================================
# Cell 3: Memory-safe SKLearn model with hashing + SGDClassifier
# ============================================================
import os, textwrap

os.makedirs("sklearn_src", exist_ok=True)

model_script = textwrap.dedent("""
import os
import argparse
import io
import json
from typing import Optional, List, Tuple

import numpy as np
import pandas as pd
import joblib

from sklearn.linear_model import SGDClassifier
from sklearn.feature_extraction import FeatureHasher
from scipy import sparse


# ------------------------
# Hashing dimensions (small enough for ml.m5.large)
# ------------------------
HASH_SIZE_TEXT = 128     # for free text fields
HASH_SIZE_CAT  = 32      # for vendor-like categorical fields


# ------------------------
# Feature hashing utilities
# ------------------------

def hash_text_series(series: pd.Series, n_features: int):
    \"""
    Hash a text column into fixed-size sparse matrix (CSR).
    \"""
    hasher = FeatureHasher(n_features=n_features, input_type="string")
    data = series.fillna("__MISSING__").astype(str).tolist()
    # Format for FeatureHasher: list of lists (each list contains tokens)
    sparse_matrix = hasher.transform([[x] for x in data])
    return sparse_matrix  # DO NOT convert to dense


def process_features(
    df: pd.DataFrame,
    is_training: bool = True,
    training_meta: Optional[dict] = None
):
    \"""
    Converts df â†’ hashed sparse CSR matrix.
    Returns:
        X (scipy.sparse CSR matrix),
        meta (dict with num/text/cat column names)
    \"""

    # Identify feature types during training
    if is_training:
        num_cols = df.select_dtypes(include=["number", "bool"]).columns.tolist()
        other_cols = [c for c in df.columns if c not in num_cols]

        # Heuristic: treat description-like fields as text
        text_cols = [c for c in other_cols if "desc" in c.lower()]
        cat_cols = [c for c in other_cols if c not in text_cols]

        meta = {
            "num_cols": num_cols,
            "text_cols": text_cols,
            "cat_cols": cat_cols
        }
    else:
        meta = training_meta
        num_cols = meta["num_cols"]
        text_cols = meta["text_cols"]
        cat_cols = meta["cat_cols"]

    # -------- Numeric features --------
    X_num = sparse.csr_matrix(df[num_cols].fillna(0).to_numpy(dtype=np.float32))

    # -------- Categorical hashed features --------
    X_cat_list = []
    for c in cat_cols:
        Xc = hash_text_series(df[c], HASH_SIZE_CAT)
        X_cat_list.append(Xc)

    # -------- Text hashed features --------
    X_text_list = []
    for c in text_cols:
        Xt = hash_text_series(df[c], HASH_SIZE_TEXT)
        X_text_list.append(Xt)

    # -------- Concatenate all parts (CSR) --------
    parts = [X_num] + X_cat_list + X_text_list
    X = sparse.hstack(parts).tocsr()

    return X, meta


# ============================================================
# Training entry point
# ============================================================

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--target-name", type=str, required=True)
    args, _ = parser.parse_known_args()

    target_name = args.target_name

    train_dir = os.environ.get("SM_CHANNEL_TRAIN", "/opt/ml/input/data/train")
    val_dir   = os.environ.get("SM_CHANNEL_VALIDATION", "/opt/ml/input/data/validation")
    model_dir = os.environ.get("SM_MODEL_DIR", "/opt/ml/model")

    train_df = pd.read_csv(os.path.join(train_dir, "train.csv"), low_memory=False)
    val_df   = pd.read_csv(os.path.join(val_dir,  "validation.csv"), low_memory=False)

    # targets
    y_train = train_df[target_name].astype(str)
    X_train_df = train_df.drop(columns=[target_name])

    y_val = val_df[target_name].astype(str)
    X_val_df = val_df.drop(columns=[target_name])

    # ---- Featurize (training) ----
    X_train, fe_meta = process_features(df=X_train_df, is_training=True)

    # ---- Featurize (validation) ----
    X_val, _ = process_features(df=X_val_df, is_training=False, training_meta=fe_meta)

    print(f"[train] X_train shape: {X_train.shape}, X_val shape: {X_val.shape}")
    print(f"[train] Memory check OK (sparse matrices).")

    # ---- Classifier (very memory efficient) ----
    clf = SGDClassifier(
        loss="log_loss",
        penalty="l2",
        alpha=1e-4,
        max_iter=20,
        n_jobs=-1,
        random_state=42
    )
    clf.fit(X_train, y_train)

    # ---- Validation Accuracy ----
    if len(y_val) > 0:
        pred = clf.predict(X_val)
        acc = (pred == y_val).mean()
        print(f"[train] Validation accuracy for {target_name}: {acc:.4f}")

    # ---- Persist model ----
    bundle = {
        "model": clf,
        "fe_meta": fe_meta,
        "target_name": target_name,
    }

    os.makedirs(model_dir, exist_ok=True)
    joblib.dump(bundle, os.path.join(model_dir, "model.joblib"))
    print("[train] Saved model bundle")


# ============================================================
# Inference handlers (MME-compatible)
# ============================================================

def model_fn(model_dir: str):
    return joblib.load(os.path.join(model_dir, "model.joblib"))


def input_fn(input_data, content_type: str):
    if content_type == "text/csv":
        if isinstance(input_data, (bytes, bytearray)):
            input_data = input_data.decode("utf-8")
        return pd.read_csv(io.StringIO(input_data), header=0)

    if content_type == "application/json":
        obj = json.loads(input_data)
        if isinstance(obj, list):
            return pd.DataFrame(obj)
        return pd.DataFrame([obj])

    raise ValueError(f"Unsupported content_type: {content_type}")


def predict_fn(data: pd.DataFrame, model_bundle):
    target = model_bundle["target_name"]
    fe_meta = model_bundle["fe_meta"]
    clf = model_bundle["model"]

    if target in data.columns:
        data = data.drop(columns=[target])

    X, _ = process_features(df=data, is_training=False, training_meta=fe_meta)
    preds = clf.predict(X)
    return preds


def output_fn(prediction, accept: str):
    if hasattr(prediction, "tolist"):
        preds = prediction.tolist()
    else:
        preds = list(prediction)

    if accept == "text/csv":
        return ",".join(str(x) for x in preds), "text/csv"

    return json.dumps(preds), "application/json"
""")

with open("sklearn_src/model_script.py", "w") as f:
    f.write(model_script)

print("Wrote sklearn_src/model_script.py")
