# ============================================================
# Cell 3: Write SKLearn train + inference script (Python 3.9 safe)
# ============================================================
import os, textwrap

os.makedirs("sklearn_src", exist_ok=True)

model_script = textwrap.dedent("""
import os
import argparse
import io
import json
from typing import Optional, List

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
import joblib


# ---------- Shared featurization helpers ----------

def _split_xy(df: pd.DataFrame, target_name: Optional[str]):
    \"""
    Split dataframe into X and y. If target_name is None, return only X.
    \"""
    if target_name is not None and target_name in df.columns:
        y = df[target_name].astype(str)
        X = df.drop(columns=[target_name])
    else:
        y = None
        X = df.copy()
    return X, y


def _featurize(
    df: pd.DataFrame,
    target_name: Optional[str] = None,
    feature_columns: Optional[List[str]] = None
):
    \"""
    Apply simple featurization:
      - Numeric columns => fill NaN with 0
      - Categorical/text => str(), fill missing tokens, one-hot encode
      - Align to training-time feature columns if provided
    \"""
    X_raw, y = _split_xy(df, target_name)

    # Identify numeric and categorical
    num_cols = X_raw.select_dtypes(include=["number", "bool"]).columns.tolist()
    cat_cols = [c for c in X_raw.columns if c not in num_cols]

    # Numeric part
    X_num = X_raw[num_cols].copy()
    X_num = X_num.fillna(0)

    # Categorical part
    if cat_cols:
        X_cat = X_raw[cat_cols].astype(str).fillna("__MISSING__")
        X_cat_dummies = pd.get_dummies(X_cat, dummy_na=False)
        X_enc = pd.concat(
            [X_num.reset_index(drop=True), X_cat_dummies.reset_index(drop=True)],
            axis=1
        )
    else:
        X_enc = X_num.reset_index(drop=True)

    # Align to known feature columns (training case defines feature_columns)
    if feature_columns is not None:
        X_enc = X_enc.reindex(columns=feature_columns, fill_value=0)
        out_cols = feature_columns
    else:
        out_cols = list(X_enc.columns)

    return X_enc, y, out_cols


# ---------- Training entrypoint ----------

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--target-name", type=str, required=True)
    parser.add_argument("--n-estimators", type=int, default=200)
    parser.add_argument("--max-depth", type=int, default=None)
    args, _ = parser.parse_known_args()

    target_name = args.target_name

    train_dir = os.environ.get("SM_CHANNEL_TRAIN", "/opt/ml/input/data/train")
    val_dir   = os.environ.get("SM_CHANNEL_VALIDATION", "/opt/ml/input/data/validation")
    model_dir = os.environ.get("SM_MODEL_DIR", "/opt/ml/model")

    train_df = pd.read_csv(os.path.join(train_dir, "train.csv"), low_memory=False)
    val_df   = pd.read_csv(os.path.join(val_dir,  "validation.csv"), low_memory=False)

    # Featurize
    X_train, y_train, feature_columns = _featurize(train_df, target_name=target_name)
    X_val,   y_val,   _ = _featurize(val_df,   target_name=target_name, feature_columns=feature_columns)

    # Train RF classifier
    clf = RandomForestClassifier(
        n_estimators=args.n_estimators,
        max_depth=args.max_depth,
        n_jobs=-1,
        random_state=42
    )
    clf.fit(X_train, y_train)

    # Optional: validation metric
    if y_val is not None and len(y_val) > 0:
        acc = (clf.predict(X_val) == y_val).mean()
        print(f"[train] Validation accuracy for {target_name}: {acc:.4f}")

    # Save model bundle
    bundle = {
        "model": clf,
        "feature_columns": feature_columns,
        "target_name": target_name,
    }

    os.makedirs(model_dir, exist_ok=True)
    joblib.dump(bundle, os.path.join(model_dir, "model.joblib"))
    print("[train] Saved model bundle")


# ---------- Inference hooks for the SKLearn SageMaker container ----------

def model_fn(model_dir: str):
    \"""
    Load the saved model bundle.
    \"""
    bundle_path = os.path.join(model_dir, "model.joblib")
    return joblib.load(bundle_path)


def input_fn(input_data, content_type: str):
    \"""
    Convert incoming request payload into pandas DataFrame.
    Supports text/csv and application/json.
    \"""
    if content_type == "text/csv":
        if isinstance(input_data, (bytes, bytearray)):
            input_data = input_data.decode("utf-8")
        return pd.read_csv(io.StringIO(input_data), header=0)

    if content_type == "application/json":
        obj = json.loads(input_data)
        if isinstance(obj, list):
            return pd.DataFrame(obj)
        return pd.DataFrame([obj])

    raise ValueError(f"Unsupported content_type: {content_type}")


def predict_fn(data: pd.DataFrame, model_bundle):
    \"""
    Apply featurization + model.predict.
    \"""
    target_name = model_bundle["target_name"]
    feature_cols = model_bundle["feature_columns"]
    clf = model_bundle["model"]

    X_enc, _, _ = _featurize(data, target_name=None, feature_columns=feature_cols)
    preds = clf.predict(X_enc)
    return preds


def output_fn(prediction, accept: str):
    \"""
    Serialize output.
    \"""
    if isinstance(prediction, (pd.Series, pd.DataFrame, np.ndarray)):
        preds = prediction.tolist()
    elif isinstance(prediction, list):
        preds = prediction
    else:
        preds = [prediction]

    if accept == "text/csv":
        return ",".join(str(x) for x in preds), "text/csv"

    return json.dumps(preds), "application/json"
""")

with open("sklearn_src/model_script.py", "w") as f:
    f.write(model_script)

print("Wrote sklearn_src/model_script.py")
