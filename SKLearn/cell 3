# ============================================================
# Cell 3: Write SKLearn train + inference script (script mode)
# ============================================================
import os, textwrap

os.makedirs("sklearn_src", exist_ok=True)

model_script = textwrap.dedent("""
import os
import argparse
import io
import json

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
import joblib

# ---------- Shared featurization helpers ----------

def _split_xy(df: pd.DataFrame, target_name: str | None):
    if target_name is not None and target_name in df.columns:
        y = df[target_name].astype(str)
        X = df.drop(columns=[target_name])
    else:
        y = None
        X = df.copy()
    return X, y

def _featurize(df: pd.DataFrame, target_name: str | None = None, feature_columns: list[str] | None = None):
    X_raw, y = _split_xy(df, target_name)

    # Separate numeric vs non-numeric
    num_cols = X_raw.select_dtypes(include=["number", "bool"]).columns.tolist()
    cat_cols = [c for c in X_raw.columns if c not in num_cols]

    X_num = X_raw[num_cols].copy()
    # Fill NaNs in numeric with 0 (you can adjust)
    X_num = X_num.fillna(0)

    if cat_cols:
        X_cat = X_raw[cat_cols].astype(str)
        X_cat = X_cat.fillna("__MISSING__")
        X_cat_dummies = pd.get_dummies(X_cat, dummy_na=False)
        X_enc = pd.concat(
            [X_num.reset_index(drop=True), X_cat_dummies.reset_index(drop=True)],
            axis=1,
        )
    else:
        X_enc = X_num.reset_index(drop=True)

    if feature_columns is not None:
        # Align to training-time feature columns
        X_enc = X_enc.reindex(columns=feature_columns, fill_value=0)
        out_cols = feature_columns
    else:
        out_cols = list(X_enc.columns)

    return X_enc, y, out_cols

# ---------- Training entrypoint ----------

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--target-name", type=str, required=True)
    parser.add_argument("--n-estimators", type=int, default=200)
    parser.add_argument("--max-depth", type=int, default=None)
    args, _ = parser.parse_known_args()

    target_name = args.target_name

    train_dir = os.environ.get("SM_CHANNEL_TRAIN", "/opt/ml/input/data/train")
    val_dir   = os.environ.get("SM_CHANNEL_VALIDATION", "/opt/ml/input/data/validation")
    model_dir = os.environ.get("SM_MODEL_DIR", "/opt/ml/model")

    train_path = os.path.join(train_dir, "train.csv")
    val_path   = os.path.join(val_dir, "validation.csv")

    print(f"[train] reading train: {train_path}")
    print(f"[train] reading val:   {val_path}")

    train_df = pd.read_csv(train_path, low_memory=False)
    val_df   = pd.read_csv(val_path, low_memory=False)

    X_train, y_train, feature_columns = _featurize(train_df, target_name=target_name)
    X_val,   y_val,   _              = _featurize(val_df,   target_name=target_name, feature_columns=feature_columns)

    print(f"[train] target='{target_name}', classes: {sorted(pd.unique(y_train))}")
    print(f"[train] X_train shape: {X_train.shape}, X_val shape: {X_val.shape}")

    clf = RandomForestClassifier(
        n_estimators=args.n_estimators,
        max_depth=args.max_depth,
        n_jobs=-1,
        class_weight=None,
        random_state=42,
    )
    clf.fit(X_train, y_train)

    # Simple metrics (for logs only)
    if y_val is not None and len(y_val) > 0:
        acc = (clf.predict(X_val) == y_val).mean()
        print(f"[train] Validation accuracy for {target_name}: {acc:.4f}")

    bundle = {
        "model": clf,
        "feature_columns": feature_columns,
        "target_name": target_name,
    }

    os.makedirs(model_dir, exist_ok=True)
    out_path = os.path.join(model_dir, "model.joblib")
    joblib.dump(bundle, out_path)
    print(f"[train] Saved model bundle to {out_path}")

# ---------- Inference hooks for SKLearn container ----------

def model_fn(model_dir: str):
    \"\"\"Load model bundle for inference (called by container).\"\"\"
    path = os.path.join(model_dir, "model.joblib")
    print(f"[model_fn] loading bundle from {path}")
    bundle = joblib.load(path)
    return bundle

def input_fn(input_data, content_type: str):
    \"\"\"Parse request body to a pandas DataFrame.\"\"\"
    if content_type == "text/csv":
        if isinstance(input_data, (bytes, bytearray)):
            input_data = input_data.decode("utf-8")
        return pd.read_csv(io.StringIO(input_data), header=0)
    elif content_type == "application/json":
        obj = json.loads(input_data)
        if isinstance(obj, list):
            return pd.DataFrame(obj)
        else:
            return pd.DataFrame([obj])
    else:
        raise ValueError(f"Unsupported content_type: {content_type}")

def predict_fn(data: pd.DataFrame, model_bundle):
    \"\"\"Apply same featurization and run model.predict.\"\"\"
    target_name = model_bundle["target_name"]
    feature_cols = model_bundle["feature_columns"]
    clf = model_bundle["model"]

    X_enc, _, _ = _featurize(data, target_name=None, feature_columns=feature_cols)
    preds = clf.predict(X_enc)
    return preds

def output_fn(prediction, accept: str):
    \"\"\"Serialize predictions.\"\"\"
    if isinstance(prediction, (pd.Series, pd.DataFrame)):
        prediction = prediction.tolist()
    elif isinstance(prediction, np.ndarray):
        prediction = prediction.tolist()

    if accept == "text/csv":
        out = ",".join(str(p) for p in prediction)
        return out, "text/csv"
    else:
        return json.dumps(prediction), "application/json"
""").strip()

with open("sklearn_src/model_script.py", "w") as f:
    f.write(model_script)

print("Wrote sklearn_src/model_script.py")
